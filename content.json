{"pages":[{"title":"","text":"In me the tiger sniffs the rose!心有猛虎，细嗅蔷薇！","link":"/about/index.html"}],"posts":[{"title":"Webflux 介绍","text":"Webflux目的与用户打交道 why created?1.基于函数式编程2.使用固定的线程和硬件资源处理并发 两种编程模型 基于注解的 controller 函数式端点Functional Endpoints Router与Handler RouterFunctions可以产生Router和Handler对象， RouterFunctions对标@Controller中的注解 Router相当于@RequestMapping Handler相当于Controller中的方法 ServerRequest和ServerResponse SpringMVC中使用的是HTTPServletRequest webFlux + SpringMVC 使用的是ServerHTTPRequest WebFlux+ 响应式 使用的是 ServerRequest 和 Spring MVC 区别IO 密集度较高，使用性能较好 使用 netty 作为 web 容器 基于注解的 WebFlux 阻塞式与响应式实现 WebFlux + SSE 服务器推 传统的SpringMVC注解与WebFlux通用，区别在于底层实现，WebFlux 中的 ServerHttpRequest 与 SpringMVC HTTPServletRequest 的区别 官网建议 如果 Spring MVC 用的好好的，不需要切换，命令式更好写，懂，debug 微服务架构，可以混合使用 如果使用了阻塞的持久化框架，如 JPA,JDBC，最好的选择就是 Spring MVC 陡峭的学习路线 处理请求类 HttpHandler 非阻塞 http 请求处理 WebHandler 高级一点，web api 的请求处理 WebSocket vs. HTTP http 只能由客户端发起连接，服务端作出响应 无状态：每次连接处理一个请求，请求结束后断开连接 无连接： 对于事务处理没有记忆能力，服务器不知道客户端是什么状态 缺陷：通信只能由客户端发起，如果服务器有连续的变化，客户端很难得知 基于http实现即时通讯 轮询: ajax 长轮询： ajax 请求，服务器 hold 住连接 长连接： 嵌入 iframe，长连接的请求，服务器不断输入数据 Flash Socket：内嵌 socket 类的 flash 程序，js 调用 flash，socket 通信 websockets 2008 诞生，2011 成为标准，浏览器支持 服务器主动向客户端推送消息，客户端也可以主动向服务器发送信息，全双工通信 建立握手连接是通过 http 传输的，建立之后，传输不需要 http 协议","link":"/2021-08-Webflux-%E4%BB%8B%E7%BB%8D/"},{"title":"MySQL系列——MySQL的执行计划","text":"基于 MySQL 8.0 community 的版本 EXPLAIN 摘要用 EXPLAIN 来可以拿到 MySQL 是怎样执行 SQL 语句的。EXPLAIN 会返回一个表，表中的每一行都表示一个 select 语句的执行的信息。 为了更生动的说明这个 MySQL 的执行计划，我用具体的例子阐述，我在本地建一个叫 school 的数据库，并建立 4 张表，分别是学生表，教师表， 分数表，课程表，写一些测试数据进去。 执行脚本文章末尾。 EXPLAIN 输出的格式说明有的测试的数据库，现在我们针对 EXPLAIN 的输出做出一些说明和描述，理解一下 MySQL 的执行计划。我们可以简单执行一下 SQL 语句：explain select * from student;输出如下： idid 就是生成的 select 查询的序号，如果 id 号相同，则从上往下执行；如果 id 不同，id 越大，优先级越高，越先被执行。比如： select_typeselect_type 主要是用来分辨查询的类型，是普通查询还是联合查询还是子查询，又有以下几种类型： SIMPLE：简单的查询，不包含子查询和 union 查询 PRIMARY：查询中若包含任何复杂的子查询，最外层查询则被标记为 PRIMARY UNION：第二个或者之后的 select 查询标记为 UNION DEPENDENT UNION：跟 UNION 类似，dependent 表示 union 或 union all 联合而成的结果会受到外部表影响 UNION RESULT：从 union 表获取结果的 select SUBQUERY：在 select 或者 where 列表中包含子查询 子查询等于一个值得时候就是 subquery DEPENDENT SUBQUERY： SUBQUERY 的子查询要受到外部表查询的影响 子查询返回的是值得集合就是 DEPENDENT SUBQUERY DERIVED：衍生，from 子句中出现的子查询，也叫派生类 DEPENDENT DERIVED：DERIVED 的子查询受到外部衍生表的影响 MATERIALIZED：物化子查询,子查询来自视图 UNCACHEABLE SUBQUERY：表示使用子查询的结果不能被换缓存 UNCACHEABLE UNION：表示使用 union 的查询的结果不能被缓存 partitions分区，分库分表才会有该值 table对应行正在访问哪一个表，表名或者别名，可能还是临时表或者 union 合并的结果集。如果是具体的表名，则是从物理表中获取的数据；如果是类似 derivedN 的形式，表示使用了 id 为 N 的查询产生的衍生表；当有 union 结果集的时候，表名是 union n1,n2 等形式，n1,n2 表示参与 union 的 id； type表示 MySQL 在表中找到所需行的方式，或者叫访问类型，常见的类型有以下几种，从上到下性能依次到好 ALL:全表扫描 index:索引全扫描，遍历整个索引来查询匹配的行主要有两种情况：一种是当前的查询时覆盖索引，即我们需要的数据在索引中就可以索取；另外一种是使用了索引进行排序，这样就避免数据的重排序； range:表示利用索引查询的时候限制了范围，在指定范围内进行查询，避免了 index 的全索引扫描。适用的操作符：=, &lt;&gt;, &gt; , &gt;=, &lt;, &lt;=, IS NULL, BETWEEN, LIKE, or IN () index_subquery:利用索引来关联子查询，不再扫描全表 unique_subquery:该连接类型类似于 index_subquery，使用的是唯一索引 index_merge:在查询过程中需要多个索引组合使用 ref_or_null:对于某个字段即需要关联条件，也需要 null 值的情况下，查询优化器会选择这种访问方式 ref:使用非唯一索引进行数据的查找，或者唯一索引的前缀扫描，返回匹配某个单独值得记录行，ref 还经常出现在 join 中。 eq_ref:使用唯一索引进行数据查找，对于每一个索引键值，表中只有一条记录匹配，简单来说就是多表连接中使用 primary key 或者 unique index 作为关联条件。 const:这个表至多有一个匹配行 system:表只有一行记录（等于系统表），这是const类型的特例 possible_keys显示可能应用在这张表中的索引，一个或者多个，查询设计到的字段上若存在索引，则列出该索引，但不一定被实际查询使用 key实际使用的索引，null 表示没有使用索引，查询中若使用了覆盖索引，则该索引和查询的 select 字段重叠 key_len表示索引中使用的字节数，可以通过 key_len 计算查询中使用的索引长度，在不损失精度的情况下长度越短越好 ref显示索引的那一列被使用了，如果可能的话，是一个常数 rows根据表的统计信息以及索引的使用情况，大致估算出找出所需记录需要读取的行数，直接反映 sql 找了多少数据，在完成目的情况下越少越好 extra包含额外的信息 using filesort: 说明 mysql 无法用索引进行排序，只能利用排序算法进行排序，会消耗额外的位置 using temporary： 建立临时表来保存中间数据，查询完成之后把临时表删除 using index： 表示当前的查询覆盖索引的，直接从索引中读取数据，而不用访问数据表，如果同时出现 using where 表明索引别用来执行索引键值的查找，如果没有，表明索引被用来读取数据，而不是真的查找 using where： 使用 where 进行条件过滤 using join buffer：使用连接缓存 impossible where： where 语句的结果总是 false 官网参考文档 Understanding the Query Execution Plan 测试表以及数据 数据库school 数据库创建 SQL >folded1CREATE DATABASE IF NOT EXISTS school DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_bin; 表测试表 >folded12345678910111213141516171819202122232425262728293031 # 学生表CREATE TABLE `student`(`student_id` VARCHAR(20),`student_name` VARCHAR(20) NOT NULL DEFAULT '',`student_birth` VARCHAR(20) NOT NULL DEFAULT '',`student_sex` VARCHAR(10) NOT NULL DEFAULT '',PRIMARY KEY(`student_id`)) ENGINE=InnoDB;# 课程表CREATE TABLE `course`(`course_id` VARCHAR(20),`course_name` VARCHAR(20) NOT NULL DEFAULT '',`teacher_id` VARCHAR(20) NOT NULL,PRIMARY KEY(`course_id`)) ENGINE=InnoDB;# 教师表CREATE TABLE `teacher`(`teacher_id` VARCHAR(20),`teacher_name` VARCHAR(20) NOT NULL DEFAULT '',PRIMARY KEY(`teacher_id`)) ENGINE=InnoDB;# 分数表CREATE TABLE `score`(`student_id` VARCHAR(20),`course_id` VARCHAR(20),`student_score` BIGINT,PRIMARY KEY(`student_id`,`course_id`)) ENGINE=InnoDB; 数据测试数据 >folded123456789101112131415161718192021222324252627282930313233343536-- 插入学生表测试数据insert into student values('01' , '赵雷' , '1990-01-01' , '男');insert into student values('02' , '钱电' , '1990-12-21' , '男');insert into student values('03' , '孙风' , '1990-05-20' , '男');insert into student values('04' , '李云' , '1990-08-06' , '男');insert into student values('05' , '周梅' , '1991-12-01' , '女');insert into student values('06' , '吴兰' , '1992-03-01' , '女');insert into student values('07' , '郑竹' , '1989-07-01' , '女');insert into student values('08' , '王菊' , '1990-01-20' , '女');-- 课程表测试数据insert into course values('01' , '语文' , '02');insert into course values('02' , '数学' , '01');insert into course values('03' , '英语' , '03');-- 教师表测试数据insert into teacher values('01' , '张三');insert into teacher values('02' , '李四');insert into teacher values('03' , '王五');-- 成绩表测试数据insert into score values('01' , '01' , 80);insert into score values('01' , '02' , 90);insert into score values('01' , '03' , 99);insert into score values('02' , '01' , 70);insert into score values('02' , '02' , 60);insert into score values('02' , '03' , 80);insert into score values('03' , '01' , 80);insert into score values('03' , '02' , 80);insert into score values('03' , '03' , 80);insert into score values('04' , '01' , 50);insert into score values('04' , '02' , 30);insert into score values('04' , '03' , 20);insert into score values('05' , '01' , 76);insert into score values('05' , '02' , 87);insert into score values('06' , '01' , 31);insert into score values('06' , '03' , 34);insert into score values('07' , '02' , 89);insert into score values('07' , '03' , 98);","link":"/2021-08-mysql-execution-plan/"},{"title":"project-reactor 框架","text":"概述真正的响应式，服务端实现，实现了 Reactive Streams 规范 what’s reactor 一个基于事件，异步处理高并发服务请求的框架，集成了 java 8的 函数式 api CompletableFuture Steam Duration 通过reactor-netty支持非阻塞进程间通信 适合微服务架构 Reactor Netty 提供 HTTP（包括 Websockets）, TCP, UDP 的背压就绪网络引擎 hot/cold cold 为每一个订阅都重新生成数据，从头开始，总能收到产生的全部数据 defer（每次订阅都是相同的返回） hot 持续不断的产生消息，订阅者只能获取订阅之后产生的消息 just share replay 提供 Flux/Mono return 一个 mono, 先返回 mono ，这个 mono 包装好各种方法， 把 方法放到调用里面，看起来是异步，实际上同步，只不过阻塞是发生在 Web 容器（Netty）里面 Router 约等于 @Controller + @RequestMapping Handler controller 里面的方法 create 可以创建 Mono, Flux，异步 generate 只能生成 Flux，同步 Disposable cancel-and-clean-up，stop producing values and clean up any resources it created Mono onNext 和 onError 不能同时用，因为最多一个 single 弹珠图 怎么阅读弹珠图","link":"/2021-08-project-reactor/"},{"title":"响应式编程","text":"what ？ 响应式宣言 一个基于面向数据流和传播变化的异步编程模型 和 Java 8 Streams 以及 Iterable-Iterator 比较 reactive 是 推模型, events 来了，推到 subscriber Java 8 Streams 以及 Iterable-Iterator 是 拉模型 背压流量控制: 告诉上游自己需要多少数据 如果不能 slow down， 只能 buffer ， drop or fail 在数据流从上游生产者向下游消费者传输的过程中，上游生产速度大于下游消费速度，导致下游的 Buffer 溢出，这种现象就叫做 Backpressure 出现。 规范标准，就是一个接口 publisher subscriber subscription processor 实现 webflux参考 webflux 文章 Project Reactor参考 Project Reactor 文章 RxJava 较老 main 里面同步 加 Scheduler，异步 响应式数据库 底层连接协议如何与数据库建立通讯 springJDBC -&gt; JDBC 规范 -&gt; mysql jdbc Driver -&gt; mysql R2DBC Drivers All terminal methods always return a Publisher type that represents the desired operation. The actual statements are sent to the database upon subscription. how总结来说，实际上就是：“异步编程，事件驱动” 消息驱动 同步与异步 callable BiFunction 同步：哪个线程产生就在哪个线程消费 命令式编程与响应式编程 函数式编程Functional Programming “what to solve” 命令式编程 “how to solve” 基于 lamda calculus 编程范式，编程程序的方法论 观察者模式 Tomcat 的 NIO 异步网络 IO 服务器推技术 Servlet 3.0 与 3.1 why ?一句话：提高性能 使用更多的线程和硬件资源来提高并行度 提高现有资源的使用效率","link":"/2021-08-reactive-programming/"},{"title":"Java JUC 中的基石： AQS 源码解读","text":"以 ReentrantLock 类为例 在使用 ReentrantLock 类时，调用 lock() 方法，我们先来看一下 lock() 方法： 12345678910111213public class ReentrantLock implements Lock, java.io.Serializable { private final Sync sync; abstract static class Sync extends AbstractQueuedSynchronizer { ... } ... public void lock() { sync.acquire(1); } ...} 看到 sync.acquire(1)，sync 是 ReentrantLock 的内部类，继承了 AbstractQueuedSynchronizer (AQS)，而 acquire(1) 方法是属于 AQS 的，我们进到里面看看： 1234567public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); }} if 条件的两个并列条件，首先是第一个 tryAcquire(1)，这个方法的作用是尝试获得锁，如果当前的锁获取不到就会执行第二个条件，尝试加入队列，如果有任何中断直接中断。先进到 tryAcquire(1) 方法看看： 12345public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); }} 这个方法没有实现，说明留给子类去做实现，这里使用到了模板方法设计模式，在 ReentrantLock 类中，默认是非公平锁，所以内部类 NonfairSync 实现了 tryAcquire(1) 方法，该类继承了 Sync，而根据上述 Sync 继承了 AQS 类。我们看来 ReentrantLock 中 的 NonfairSync 类的结构： 12345678910public class ReentrantLock implements Lock, java.io.Serializable { static final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } }} NonfairSync 类里面又调用父类 Sync 的 nonfairTryAcquire(1)，至此，我们到了获取锁的核心方法。 1234567891011121314151617181920212223242526272829public class ReentrantLock implements Lock, java.io.Serializable { abstract static class Sync extends AbstractQueuedSynchronizer { private static final long serialVersionUID = -5179523762034025860L; /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ @ReservedStackAccess final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); // 获取当前线程 int c = getState(); // 获取当前状态，这个方法是 AQS 里面的方法，拿到的是 volatile 的 state 值，具体 state 值怎么用是子类要干的事情 if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } }} 拿到 state 值以后，if 条件中，判断 state 的值，如果 state 值为 0 ，说明还没有线程拿到锁，然后 CAS 齐昂锁，如果抢锁成功，那么 state 值为 1，并且把当前线程设置为独占锁；如果 state 值不是 0， 说明已经有其他的线程占用了这个锁；else if 的条件分支中，再判断一下已经拿到这个锁的是不是自己，如果是自己的话，就把 state + 1，释放的时候会 - 1。在判断 state 值既不是 0，也不是当前线程持有锁，那么一定是其他线程正在持有锁，返回 false。那么在 AbstractQueuedSynchronizer 类中 acquire(1)方法的第一个 if 条件 tryAcquire(1) 是 false，说明 ReentrantLock实例在调用 lock() 方法没有拿到锁，那就执行if 并列条件的第二个方法 acquireQueued(addWaiter(Node.EXCLUSIVE), arg))，这个方法是尝试加入到等待队列中。首先我们来看 addWaiter() 方法： 12345678910111213141516171819public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { private Node addWaiter(Node mode) { Node node = new Node(mode); for (; ; ) { Node oldTail = tail; if (oldTail != null) { node.setPrevRelaxed(oldTail); if (compareAndSetTail(oldTail, node)) { oldTail.next = node; return node; } } else { initializeSyncQueue(); } } }} 这里我们看到这个方法返回 Node 类，这个 Node 类是什么呢？我们看一下 Node 类的定义： 点击展开: AQS 的内部类 Node 类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { /** * Wait queue node class. * * &lt;p&gt;The wait queue is a variant of a &quot;CLH&quot; (Craig, Landin, and * Hagersten) lock queue. CLH locks are normally used for * spinlocks. We instead use them for blocking synchronizers, but * use the same basic tactic of holding some of the control * information about a thread in the predecessor of its node. A * &quot;status&quot; field in each node keeps track of whether a thread * should block. A node is signalled when its predecessor * releases. Each node of the queue otherwise serves as a * specific-notification-style monitor holding a single waiting * thread. The status field does NOT control whether threads are * granted locks etc though. A thread may try to acquire if it is * first in the queue. But being first does not guarantee success; * it only gives the right to contend. So the currently released * contender thread may need to rewait. * * &lt;p&gt;To enqueue into a CLH lock, you atomically splice it in as new * tail. To dequeue, you just set the head field. * &lt;pre&gt; * +------+ prev +-----+ +-----+ * head | | &lt;---- | | &lt;---- | | tail * +------+ +-----+ +-----+ * &lt;/pre&gt; * * &lt;p&gt;Insertion into a CLH queue requires only a single atomic * operation on &quot;tail&quot;, so there is a simple atomic point of * demarcation from unqueued to queued. Similarly, dequeuing * involves only updating the &quot;head&quot;. However, it takes a bit * more work for nodes to determine who their successors are, * in part to deal with possible cancellation due to timeouts * and interrupts. * * &lt;p&gt;The &quot;prev&quot; links (not used in original CLH locks), are mainly * needed to handle cancellation. If a node is cancelled, its * successor is (normally) relinked to a non-cancelled * predecessor. For explanation of similar mechanics in the case * of spin locks, see the papers by Scott and Scherer at * http://www.cs.rochester.edu/u/scott/synchronization/ * * &lt;p&gt;We also use &quot;next&quot; links to implement blocking mechanics. * The thread id for each node is kept in its own node, so a * predecessor signals the next node to wake up by traversing * next link to determine which thread it is. Determination of * successor must avoid races with newly queued nodes to set * the &quot;next&quot; fields of their predecessors. This is solved * when necessary by checking backwards from the atomically * updated &quot;tail&quot; when a node's successor appears to be null. * (Or, said differently, the next-links are an optimization * so that we don't usually need a backward scan.) * * &lt;p&gt;Cancellation introduces some conservatism to the basic * algorithms. Since we must poll for cancellation of other * nodes, we can miss noticing whether a cancelled node is * ahead or behind us. This is dealt with by always unparking * successors upon cancellation, allowing them to stabilize on * a new predecessor, unless we can identify an uncancelled * predecessor who will carry this responsibility. * * &lt;p&gt;CLH queues need a dummy header node to get started. But * we don't create them on construction, because it would be wasted * effort if there is never contention. Instead, the node * is constructed and head and tail pointers are set upon first * contention. * * &lt;p&gt;Threads waiting on Conditions use the same nodes, but * use an additional link. Conditions only need to link nodes * in simple (non-concurrent) linked queues because they are * only accessed when exclusively held. Upon await, a node is * inserted into a condition queue. Upon signal, the node is * transferred to the main queue. A special value of status * field is used to mark which queue a node is on. * * &lt;p&gt;Thanks go to Dave Dice, Mark Moir, Victor Luchangco, Bill * Scherer and Michael Scott, along with members of JSR-166 * expert group, for helpful ideas, discussions, and critiques * on the design of this class. */ static final class Node { /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; /** waitStatus value to indicate thread has cancelled. */ static final int CANCELLED = 1; /** waitStatus value to indicate successor's thread needs unparking. */ static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition. */ static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate. */ static final int PROPAGATE = -3; /** * Status field, taking on only the values: * SIGNAL: The successor of this node is (or will soon be) * blocked (via park), so the current node must * unpark its successor when it releases or * cancels. To avoid races, acquire methods must * first indicate they need a signal, * then retry the atomic acquire, and then, * on failure, block. * CANCELLED: This node is cancelled due to timeout or interrupt. * Nodes never leave this state. In particular, * a thread with cancelled node never again blocks. * CONDITION: This node is currently on a condition queue. * It will not be used as a sync queue node * until transferred, at which time the status * will be set to 0. (Use of this value here has * nothing to do with the other uses of the * field, but simplifies mechanics.) * PROPAGATE: A releaseShared should be propagated to other * nodes. This is set (for head node only) in * doReleaseShared to ensure propagation * continues, even if other operations have * since intervened. * 0: None of the above * * The values are arranged numerically to simplify use. * Non-negative values mean that a node doesn't need to * signal. So, most code doesn't need to check for particular * values, just for sign. * * The field is initialized to 0 for normal sync nodes, and * CONDITION for condition nodes. It is modified using CAS * (or when possible, unconditional volatile writes). */ volatile int waitStatus; /** * Link to predecessor node that current node/thread relies on * for checking waitStatus. Assigned during enqueuing, and nulled * out (for sake of GC) only upon dequeuing. Also, upon * cancellation of a predecessor, we short-circuit while * finding a non-cancelled one, which will always exist * because the head node is never cancelled: A node becomes * head only as a result of successful acquire. A * cancelled thread never succeeds in acquiring, and a thread only * cancels itself, not any other node. */ volatile Node prev; /** * Link to the successor node that the current node/thread * unparks upon release. Assigned during enqueuing, adjusted * when bypassing cancelled predecessors, and nulled out (for * sake of GC) when dequeued. The enq operation does not * assign next field of a predecessor until after attachment, * so seeing a null next field does not necessarily mean that * node is at end of queue. However, if a next field appears * to be null, we can scan prev's from the tail to * double-check. The next field of cancelled nodes is set to * point to the node itself instead of null, to make life * easier for isOnSyncQueue. */ volatile Node next; /** * The thread that enqueued this node. Initialized on * construction and nulled out after use. */ volatile Thread thread; /** * Link to next node waiting on condition, or the special * value SHARED. Because condition queues are accessed only * when holding in exclusive mode, we just need a simple * linked queue to hold nodes while they are waiting on * conditions. They are then transferred to the queue to * re-acquire. And because conditions can only be exclusive, * we save a field by using special value to indicate shared * mode. */ Node nextWaiter; /** * Returns true if node is waiting in shared mode. */ final boolean isShared() { return nextWaiter == SHARED; } /** * Returns previous node, or throws NullPointerException if null. * Use when predecessor cannot be null. The null check could * be elided, but is present to help the VM. * * @return the predecessor of this node */ final Node predecessor() { Node p = prev; if (p == null) throw new NullPointerException(); else return p; } /** Establishes initial head or SHARED marker. */ Node() {} /** Constructor used by addWaiter. */ Node(Node nextWaiter) { this.nextWaiter = nextWaiter; THREAD.set(this, Thread.currentThread()); } /** Constructor used by addConditionWaiter. */ Node(int waitStatus) { WAITSTATUS.set(this, waitStatus); THREAD.set(this, Thread.currentThread()); } /** CASes waitStatus field. */ final boolean compareAndSetWaitStatus(int expect, int update) { return WAITSTATUS.compareAndSet(this, expect, update); } /** CASes next field. */ final boolean compareAndSetNext(Node expect, Node update) { return NEXT.compareAndSet(this, expect, update); } final void setPrevRelaxed(Node p) { PREV.set(this, p); } // VarHandle mechanics private static final VarHandle NEXT; private static final VarHandle PREV; private static final VarHandle THREAD; private static final VarHandle WAITSTATUS; static { try { MethodHandles.Lookup l = MethodHandles.lookup(); NEXT = l.findVarHandle(Node.class, &quot;next&quot;, Node.class); PREV = l.findVarHandle(Node.class, &quot;prev&quot;, Node.class); THREAD = l.findVarHandle(Node.class, &quot;thread&quot;, Thread.class); WAITSTATUS = l.findVarHandle(Node.class, &quot;waitStatus&quot;, int.class); } catch (ReflectiveOperationException e) { throw new ExceptionInInitializerError(e); } } }} 总结一下就是，上面说到的等待队列是由一个双向链表实现的，这个队列是 CLH 队列的变种，把 synchronized 换成了 CAS， 而 Node 类就是这个双向链表带有线程信息的节点类。 Node 只有两种模式，EXCLUSIVE 和 SHARED，还有个成员变量 waitStatus，有几种状态，注释文档说的很清楚，最重要的是 SIGNAL 状态，表示当前的线程的后继节点正在阻塞等待， 当前线程释放锁或者取消后需要唤醒它的后继节点。 > 为什么是双向链表，单向的行不行？ 行，但是还是不够好，如果我要找到某个 node 的前一个节点，时间复杂度就是 O(n)，如果是双向链表就是 O(1)。 继续看 addWaiter(Node.EXCLUSIVE)方法，首先拿到等待队列的 tail 节点，如果为空就初始化一个队列，头尾都是指向这个 node；如果 tail 存在，就把要添加的这个 node 的 prev 指向 tail 节点，因为在操作的过程中，可能其他的线程改动了 tail，所以需要CAS 自旋的把 tail 节点的 next 指向这个要添加的 node。一句话就是：addWaiter() 方法就是添加 node 到等待队列的队尾。然后返回这个 node 添加到队尾之后，执行 acquireQueued(Node.EXCLUSIVE,1)方法，再看一下这个方法： 1234567891011121314151617181920212223242526272829303132public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { /** * Acquires in exclusive uninterruptible mode for thread already in * queue. Used by condition wait methods as well as acquire. * * @param node the node * @param arg the acquire argument * @return {@code true} if interrupted while waiting */ final boolean acquireQueued(final Node node, int arg) { boolean interrupted = false; try { for (; ; ) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC return interrupted; } if (shouldParkAfterFailedAcquire(p, node)) interrupted |= parkAndCheckInterrupt(); } } catch (Throwable t) { cancelAcquire(node); if (interrupted) selfInterrupt(); throw t; } }} 首先去拿这个 node 的 prev 节点，先判断一下是不是头结点，然后再去 tryAcquire(1) ，看看能不能拿到锁，万一头结点刚好释放锁呢，拿到锁之后，说明头结点释放了锁，把这个 node 设置为头结点，然后头结点的 next 节点设置为 null，这样头结点就不会有引用存在，帮助 GC 回收，如果有中断就返回了。如果说，这个 node 的 prev 节点不是头结点或者没有拿到锁，那么进入下面一个条件判断，进入方法 shouldParkAfterFailedAcquire()： 123456789101112131415161718192021222324252627282930313233343536373839public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { /** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev. * * @param pred node's predecessor holding status * @param node the node * @return {@code true} if thread should block */ private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ pred.compareAndSetWaitStatus(ws, Node.SIGNAL); } return false; }} 在看这个方法前，我们看一下参数，进到这个方法的前提是，如果参数 pred 不是头结点，当前线程也没有拿到锁，那么是不是应该等一下？首先拿到 pred 这个 node 的状态，判断：如果 pred 这个 node 的状态是 SINGAL，表示 pred 的这个 node 待会释放锁的时候会唤醒后继节点，也就是参数 node 指向的这个 Node，实际上就是当前的 node，那么返回 true，就是要等一会；如果 pred 这个 node 的状态是大于 0，大于 0 的状态是 CANCELLED 的状态，可能这个线程 node 被取消调度或者timeout，那么就再去找 pred 的这个 node 的前驱节点，反正一直找到不是 CANCELLED 状态的节点；如果 pred 这个 node 的状态是小于等于 0， waitStatus 默认是 0，小于 0 是处在 CONDITION 和 PROPAGATE 的状态，把 pred 的这个 node 的状态 CAS 设置成 SIGNAL 状态，最后返回 false。总结一下，实际上就是当前的线程节点加塞成为即将被唤醒的节点，坏得很~ 我们再回到方法 acquireQueued(Node.EXCLUSIVE, 1) 中，如果 shouldParkAfterFailedAcquire() 返回是 true，那么就是执行 parkAndCheckInterrupt()，即： 123456789101112public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { /** * Convenience method to park and then check if interrupted. * * @return {@code true} if interrupted */ private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); }} 这个方法就是调用同步辅助工具类 LockSupport.park(this)，阻塞住当前线程。 最后，如果在 lock() 的整个过程中拿到了锁，就会继续执行后面的程序，如果没有就阻塞住，这就是整个 AQS 的源码基本思路。 我们再来关注一下 AQS 为什么效率高？主要是 AQS 采用 CAS 来操作链表尾巴，如果好多线程都要往链表尾巴上插入节点，第一想法肯定会加锁，锁定整个 (Sync) 对象，保证线程安全，但是锁定整个链表的话，锁的太多太大了，现在 AQS 并不是锁定整个链表的方法，而是观测 tail 这个节点就可以了，用 CAS 是做实现，这就是AQS 效率高的核心。","link":"/2021-09-Java-JUC-%E4%B8%AD%E7%9A%84%E5%9F%BA%E7%9F%B3%EF%BC%9A-AQS-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"},{"title":"Java JUC 包中原子类源码分析 CAS","text":"以 AtomicInteger 为例。 在使用 AtomicInteger 类时，加1操作会调用方法 incrementAndGet(),这个方法就是 CAS 的实现。先瞥一眼这个方法的内容， 实际上就是拿到 value 的值，然后再执行加 1 的操作。 1234567891011121314151617public class AtomicInteger extends Number implements java.io.Serializable { private static final jdk.internal.misc.Unsafe U = jdk.internal.misc.Unsafe.getUnsafe(); /** * Unsafe 类直接操作内存 * 这个 VALUE 的值就是 AtomicInteger 类 value 的内存位置地址的偏移量， * 通过 native 方法，拿到地址指针 */ private static final long VALUE = U.objectFieldOffset(AtomicInteger.class, &quot;value&quot;); private volatile int value; public final int incrementAndGet() { return U.getAndAddInt(this, VALUE, 1) + 1; } // 省略其他方法} 再跟踪到 getAndAddInt() 方法中，进到了 Unsafe 类中，如下： 123456789101112public final class Unsafe { ... @HotSpotIntrinsicCandidate public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!weakCompareAndSetInt(o, offset, v, v + delta)); return v; } ...} 这个就是 CAS 的核心实现，流程如下： do 里面，先去拿 this 对象在偏移量为 offset 的值，v 取到的是 0； 然后 while 中 weakCompareAndSetInt() 方法再去判断这个偏移量位置上是不是 0，也就是期望的应该是0， 2.1 如果是 0，就把 0 + 1 = 1 写进去； 2.2 如果不是 0，说明在取到 0 后，有其他线程修改了这个 offset 位置的值，比如 1 2.3 然后又执行一次 do 里面的操作，拿到 v = 1，再去看看这个 offset 位置上的值是不是1，是就加上1，不是继续 do..while 操作 2.4 成功了就结束。 然后返回了 v，注意，这个 v 的值还是刚才取到的 0，并没有重新去取。 这里有个问题，有没有可能这样的情况出现，do 里面取到了是 0，然后比较了当前的位置确实是 0 ，然后再写的过程值又发生了变化呢？ 答案是不可能。我们进到 weakCompareAndSetObject(Object o, long offset,Object expected,Object x) 方法里面看一下源代码： 12345678910111213141516171819public final class Unsafe { @HotSpotIntrinsicCandidate public final boolean weakCompareAndSetObject(Object o, long offset, Object expected, Object x) { return compareAndSetObject(o, offset, expected, x); } /** * 如果当前持有expected则以原子方式将 Java 变量更新为x * 此操作具有volatile读写的内存语义。 对应于 C11 atomic_compare_exchange_strong。 * 返回：如果成功则为true */ @HotSpotIntrinsicCandidate public final native boolean compareAndSetInt(Object o, long offset, int expected, int x);} 我们可以看到注释里面 atomic_compare_exchange_strong 函数，这是 C11 原子的函数，反映到处理器的指令上就是 CMPXCHG 指令，这条指令已经无法再分割，而这条指令执行的时候，通过锁总线来让其他核心处理器都不能访问这个地址。简单来说，从 CPU 原语的级别来保证了 CAS 的操作。","link":"/2021-09-Java-JUC-%E5%8C%85%E4%B8%AD%E5%8E%9F%E5%AD%90%E7%B1%BB%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"Java内存模型之what,why,how","text":"why&emsp;&emsp;我们先来聊聊一些背景知识，首先现代的计算机中，多任务和高并发是衡量一台计算机处理的能力重要指标之一，一个关键性的指标 TPS（transaction per second），一般能说明问题，它代表的是一秒内服务器平均能响应的请求数。&emsp;&emsp;其次，硬件的效率和一致性的方面，计算机的处理器运算速度与存储设备（内存，硬盘）的读写速度有几个数量级的差距，为了缓解这种差距，现在计算机加入高速缓存（cache）来作为内存与处理器之间的缓冲，其读写速度接近于 CPU，将运算需要的数据复制到缓存中，运算结束后在动缓存同步到内存中。但是事务的发展总是解决老问题带来新问题，高速缓存解决了处理器与内存之间的速度矛盾，带来了一个新的问题，缓存一致性（Cache Coherence）。 &emsp;&emsp;尤其是在多处理器的系统中，每个处理器都有自己的高速缓存，而他们有共享同一个主存。多处理器系统缓存示意图和多级缓存结构如下： &emsp;&emsp;所以需要一种协议可以保障数据的一致性，目前主流的缓存一致性协议有 MSI, MESI, MOSI, Dragon Protocol, 要注意的是缓存都是有一个基础单位 cache line，这也是java中造成伪共享的根本原因。 &emsp;&emsp;值得注意的是，为了处理器内部的运算单元被充分利用，处理器可能会对代码进行乱序执行（out of order execution） 优化，在 java 中，JIT 也是有执行重排序的优化，经典案例就是双重检查单例，要不要加 volatile 关键字。 what所谓的 java 的内存模型的概念主要有以下几点： 主要目标是定义各个变量的访问规则，即 jvm 中将变量存储到内存和从内存中取出变量这样底层细节 规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存 （类比于处理器的高速缓存） 线程的工作内存中保存了该线程使用到的变量到主内存副本拷贝 线程对变量的所有操作（读取、赋值）都必须在工作内存中进行，而不能直接读写主内存中的变量 不同线程之间无法直接访问对方的工作内存中的变量，线程间变量的传递需要在主存中来完成 how最后，我们知道了 java 内存模型的一些定义和条件，那么怎样实现它的呢？主要有以下的几点： 8种操作 lock unlock read load use assign store write happens-before 原则 重排序从 java 源代码到最终实际执行的指令序列，经过三种排序 synchronized volatile final &emsp;&emsp;综上所述，我们对 java 内存模型的背景、概念、实现的过程有个总体上的理解。每一部分涉及到的细节还是很多的，以后再花时间深究。","link":"/2021-09-Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B9%8Bwhat-why-how/"},{"title":"Kubernetes介绍","text":"K8S 是什么Kubernetes，又称为 k8s（首字母为 k、首字母与尾字母之间有 8 个字符、尾字母为 s，所以简称 k8s）或者简称为 “kube”。一个开源平台，用来干嘛的？用来自动化部署，缩扩容和管理容器应用的。下面这张图是 K8S 集群的架构图。 基本概念和术语 预期状态管理（Desired State Management） K8S API 对象（声明预期状态） ConfigMap why解耦配置文件和容器 典型用法 作为环境变量，为容器所使用 设置容器启动命令的启动参数（需要设置为环境变量） 以 Volume 的形式挂载为容器内部的文件或者目录 创建 yaml 文件，kubectl create -f configmap.yaml 命令行格式：kubectl create configmap --from-file=[key=]source --from-file=[key=]source（1）–from-file=文件名，就是从文件中创建（2）–from-file=文件夹名，就是目录中创建（3）–from-file=key=value 键值对，就是直接从 key-value 键值对创建 使用 通过环境变量的方式使用 通过 volumeMount 的方式使用 限制必须在 pod 创建之前完成 K8S Control Plane确保集群当前状态匹配预期状态 工作负载 Pod最小的调度和计算单位，由一个或者多个 container 组成，Pod 中的所有 container 共享存储，网络空间和 cgroup 的资源 工作负载资源（Controllers） Deployment &amp; ReplicaSetDeployment 部署无状态应用，可以 rollout、update、rollback ReplicaSet。 ReplicaSet 保证特定数量的 Pod 副本始终保持运行状态，ReplicaSet 并不是直接部署，而是通过 Deployment 来部署。 StatefulSetStatefulSet 部署有状态的应用，StatefulSet 要求一个 headless Service 搭配使用。 为什么要用 headless service + StatefulSet 部署有状态应用？ headless service会为关联的Pod分配一个域&lt;service name&gt;.$&lt;namespace name&gt;.svc.cluster.local StatefulSet会为关联的Pod保持一个不变的Pod Namestatefulset中Pod的hostname格式为$(StatefulSet name)-$(pod序号) StatefulSet会为关联的Pod分配一个dnsName$&lt;Pod Name&gt;.$&lt;service name&gt;.$&lt;namespace name&gt;.svc.cluster.local DaemonSet保证所有或者部分 Nodes 运行 Pod 的副本。使用场景在集群存储进程，日志收集进程，节点监控进程 Job &amp; CronJobJob 和 CronJob 都是任务，前者是直到特定数量的 Job 被成功执行，后者是重复间隔执行一次。 核心组件的运行机制 K8S Master kube-apiserver (API Server) 对外提供各种对象的CRUD REST接口 对外提供Watch机制，通知对象变化 将对象存储到 Etcd 中 kube-controller-mamager (守护进程)通过 apiserver 监视集群的状态，并做出相应更改，以使得集群的当前状态向预期状态靠拢，通过 Controller 会尝试将状态调整为期望的状态。Controller 包括：Replication Controller，Node Controller，ResourceQuota Controller，Namespace Controller，ServiceAccount Controller，Token Controller，Service Controller，Endpoint Controller。 kube-scheduler (调度器)将待调度的 Pod 按照特定的调度算法和调度策略绑定到集群中某个适合的 Node 上，并将绑定信息写入到 etcd 中，调度需要考虑的因素有：资源需求，服务治理要求，硬件/软件/策略限制，亲和以及反亲和要求，数据局域性，负载间的干扰等。 Work Node Kubelet（节点代理）接受通过各种机制（主要是通过 apiserver ）提供的一组 PodSpec，确保 PodSpec 中描述的容器处于运行状态且运行状况良好 Kube-proxy（节点网络代理）在节点上提供 Kubernetes API 中定义 Service，设置Service对应的 IPtables 规则，进行流量转发（userspace模式） 共享存储原理 PV(PersistentVolume)它是对底层网络存储的抽象，由管理员创建和配置，挂载到 node 上。PV 的生命周期： available资源的产生来源： 静态模式管理员手动创建许多 PV 动态模式通过 StorageClass 的设置对后端存储进行描述，标记为某种类型 Bound Released Failed PVC(PersistentVolumeClaim)对 PV 的 一个申请，就像 pod 消费 node 资源一样，PVC StorageClass 生命为 “”，说明PVC禁止使用动态模式，多个 pod 可以挂载同一个 PVC StorageClass管理员定义储存资源为某种类别，用户根据 storage class 直观了解存储资源的特性 CSI 机制 （Container Storage Interface）容器存储接口 why ?上面的方式都是 in-tree，耦合太重 what ?与容器对接的存储接口标准，存储提供方只需要基于接口标准进行存储插件的实现，就能使用K8S 的原生存储机制为容器提供存储 动态存储 安全机制 基础概念User类型： Service AccountK8S 管理它，并且绑定在特定的 namespace Normal User存在于 K8S 集群外，使用集群的 CA 证书授权 Authentication — 客户端认证 最严格的 HTTPS 正数认证： 基于 CA 根证书签名的双向数字证书认证方式 HTTP token 认证： 通过一个 Token 来识别用户 HTTP base 认证： 通过用户名 + 密码的方式认证 Authorization — 授权 RBAC Namespace Scope Role RoleBinding：把一个角色绑定到一个目标上 User，Group，Service Account Cluster Scope ClusterRole CluseterRoleBinding：只对集群级别角色生效 授权策略来决定一个 API 调用是否合法 API Server 授权策略 AlawaysDeny: 拒绝所有请求，用于测试 AlawaysAllow: K8S 默认设置，允许接收所有请求，集群不需要授权流程，可以采用 ABAC (Attribute-Based Access Control): 基于属性的访问控制，使用用户配置的授权规则对用户请求进行匹配和控制 RBAC (Role-Base Access Control): 基于角色的访问控制 Node: 专用模式，对 kubelet 发出的请求进行访问控制 Webhook: 调用外部 REST 服务对用户进行授权鉴权的流程，可以通过命令看详细的请求，kubectl --v=8 version，流程如下图： Admission Control — 准入控制准入控制器的插件列表中的每个准入控制器检查请求，插件举例：LimitRanger，ServiceAccount 等 Service Account — 服务账户不是给用户用的，给运行在 Pod 中的进程提供必要的身份证明，与API Server 交互流程为： Pod 访问 API Server 的服务时，以 service 方式调用名为 Kubernetes 的这个服务的，而 Kubernetes 服务只在HTTPS 安全端口 443 上提供 请求时，类似于 HTTP Token，Header 中加了 token 字符串，来自于 /run/secrets/kubernetes.io/serviceaccount/token 建立链接时，用 Pod 里指定路径下的一个 CA 证书 （/run/secrets/kubernetes.io/serviceaccount/ca.rt)，验证 API Server 发来的证书 API Server 在收到这个 Token，会对这个 token 进行合法性验证 /run/secrets/kubernetes.io/serviceaccount/namespace 会作为参数调用 Kubernetes API 每个 Namespace 下都有个 default 的默认 Service Account 对象，名为 Tokens 的可以当作 Volume 被挂载到 Pod 里的 Secret，Pod 启动时挂载，协助进程访问 API Server 时的鉴权。 Secret — 私密凭据保管私密凭据，从属于 Service Account，一旦 Secret 的使用方式： 创建 Pod 时，为 Pod 指定 Service Account 自动使用改 Secret 挂载该 Secret 到 Pod 使用，执行 ls /etc/{secret name} 可以看到 Secret 的 data 域中的 key 值作为目录中文件 在 Docker image 下载时，指定 Pod 的 spec.ImagePullSecrets 网络原理K8S 的网络原理比较复杂，基于 Docker 的网络之上有封装很多东西，深入了解还需要时间。 基础概念需要了解网络里面的一些概念和原理 Linux Network Virtualization， Linux Tun/Tap Linux Network Namespace Veth Pair Linux Bridge Vlan VxlAN Routing ProtocolDistance Vector Protocol,BGPLink-State Protocol,OSPF K8S Network Service Cluster IP Headless NodePort LoadBalancer Ingress K8S Ingress lstio Ingress Gateway API Gateway + Service Mesh Kubernetes CNI 插件, Calico","link":"/2021-09-Kubernetes%E4%BB%8B%E7%BB%8D/"},{"title":"线程池源码分析","text":"线程池及简单介绍（1）线程池类关系图 Executors 是对线程执行的工具类，可以看做是线程池的工厂。execute() 方法任务立即执行，submit() 方法提交任务等待线程自己调度运行。 （2）线程池核心参数的交互 corePoolSize 核心线程数 maximumPoolSize 最大线程数 keepAliveTime 生存时间，线程池中超过核心线程数大小的线程的存活时间，如果设置 allowCoreThreadTimeOut 为 true，那么核心线程数也是此时间的存活时间 unit 生存时间的单位，类型是 TimeUnit workQueue 任务队列 BlockingQueue&lt;Runnable&gt; threadFactory 线程工厂 ThreadFactory handler 拒绝策略， 类型是 RejectedExecutionHandler 首先线程池中会保持 corePoolSize 大小的线程运行，即使没有任务也会空闲运行，但是如果设置了 allowCoreThreadTimeOut = true，那么核心线程也会在 keepAliveTime 时间大小之后关闭。当任务超过了核心线程数的话，会把新的任务放到工作队列，如果工作队列满了，并且当前的工作线程是小于 maximumPoolSize 定义的最大线程数的，那么创建一个新线程来运行任务，当运行的线程数超过了最大线程数的值，拒绝策略开始发挥作用，默认是丢弃策略。 终于来到了我们的线程池的源码解析啦，因为我们知道jdk自带的各种线程池本质上都是核心类 ThreadPollExecutor 构造出来的，所以我们来看里面到到底是个什么鬼？ 源码分析 成员变量 123456789101112131415161718192021222324252627282930313233public class ThreadPoolExecutor extends AbstractExecutorService { // 高 3 位表示线程池状态，低29位表示 worker 的数量 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static final int COUNT_BITS = Integer.SIZE - 3; // 32 - 3 = 29 /** * 0010 0000 0000 0000 0000 0000 0000 0000 COUNT_BITS 二进制值 - 0000 0000 0000 0000 0000 0000 0000 0001 1 二进制值 ———————————————————————————————————————————— = 0001 1111 1111 1111 1111 1111 1111 1111 COUNT_MASK 二进制值 * */ private static final int COUNT_MASK = (1 &lt;&lt; COUNT_BITS) - 1; // 1 * 2^29 -1 // runState is stored in the high-order bits private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; // 表示接收新任务和处理队列中的任务 private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; // 表示不接收新任务了，但是会处理队列中的任务 private static final int STOP = 1 &lt;&lt; COUNT_BITS; // 表示不接收新任务，也不处理队列中的任务，中断进行中的任务 private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; // 如果所有的任务的已经结束，工作线程是0，此时的线程状态转变为 TIDYING，调用 terminated() 钩子方法 private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; // 表示 terminated() 方法执行完成 // Packing and unpacking ctl // ~COUNT_MASK 值: 1110 0000 0000 0000 0000 0000 0000 0000 // 按位与，低 29 位都是 0 private static int runStateOf(int c) { return c &amp; ~COUNT_MASK; } // COUNT_MASK 二进制值 : 0001 1111 1111 1111 1111 1111 1111 1111 // 按位与，高 3 位都是 0 private static int workerCountOf(int c) { return c &amp; COUNT_MASK; } // 根据线程池状态和线程数量计算 control(ctl) 的值 private static int ctlOf(int rs, int wc) { return rs | wc; }} ctl 这个原子整形变量，包含了两个含义，一个是 workerCount，就是线程池里面 worker 数量，另一个是 runState，就是线程池状态，运行还是停止等等。咦，一个变量怎么表示的两种意思呢？有一个很巧妙的方法就是高3 位表示线程池的状态，低 29 位来表示线程池中的线程数量，目前来说是29位数量足够的，如果不够，后面会扩展成 Long 类型就可以，这样在高并发的情况减少变量的锁同步。具体怎么做到的呢？先来看看线程池状态的表示，首先是 RUNNING 状态： private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; 我们知道 COUNT_BITS 是 29， -1 的二进制值表示是 1 的二进制值，取反，然后加 1 ，就是 -1 的二进制表示。int 是 32 位，那么： 0000 0000 0000 0000 0000 0000 0000 0001 1 的二进制1111 1111 1111 1111 1111 1111 1111 1110 取反1111 1111 1111 1111 1111 1111 1111 1111 加 1 即为 -1 的二进制表示 RUNNING 状态是左移 29 位，那么： 1111 1111 1111 1111 1111 1111 1111 1111 -1 的二进制表示1110 0000 0000 0000 0000 0000 0000 0000 左移 29 后的值 即 RUNNING 的值是：1110 0000 0000 0000 0000 0000 0000 0000， 同理得到：SHUTDOWN&emsp;&nbsp; 的值是 0000 0000 0000 0000 0000 0000 0000 0000，STOP&emsp;&emsp;&emsp;&nbsp; 的值是 0010 0000 0000 0000 0000 0000 0000 0000，TIDYING&emsp;&nbsp;&nbsp;&nbsp; 的值是 0100 0000 0000 0000 0000 0000 0000 0000，TERMINATED&nbsp; 的值是 0110 0000 0000 0000 0000 0000 0000 0000。 所以 ctl 变量的默认值是 RUNNING 的值 和 0 或运算，即 ctl = 1110 0000 0000 0000 0000 0000 0000 0000，实际上意思就是线程池在 RUNNING 状态，但是 0 个工作线程。通过方法 runStateOf() 可以拿到当前线程池的状态值。 上面说的是线程池状态的表示，再来看一下线程池中线程的个数，变量 COUNT_MASK 就是表示线程数的大小，最多（2^29 - 1） 个，通过方法 workerCountOf() 可以计算线程的个数。 构造方法构造方法主要是进行一些非空判断和校验。 1234567891011121314151617181920212223public class ThreadPoolExecutor extends AbstractExecutorService { public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); // 将存活时间转换成纳秒 this.threadFactory = threadFactory; this.handler = handler; }} 提交任务的过程提交任务时，会调用 execute() 方法，我们来看一下这个方法： 12345678910111213141516171819202122232425262728293031323334353637383940public class ThreadPoolExecutor extends AbstractExecutorService { public void execute(Runnable command) { if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn't, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) // 带了任务参数 return; c = ctl.get(); } if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (!isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); // 没有带任务参数 } else if (!addWorker(command, false)) reject(command); }} execute() 方法的思路注释文档说的很清楚，首先非空判断，然后拿到 ctl 的值，来判断： 值（默认是 0 ）小于核心线程数的如果是小于就添加一个 Worker，这个 Worker 是 ThreadPoolExecutor 的内部类，简单来讲就是封装了线程信息和任务信息的类，后面再谈。 值大于核心线程数， 2.1 再判断线程池状态是 RUNNING 状态，就添加到工作队列当中去 2.1.1 这里注意的是还会再一次检查线程池的状态，以防线程池的状态在添加队列的过程中发生改变，如果发现线程池不是 RUNNING 状态的话，那么就把队列里面的任务 remove 掉，然后调用拒绝策略来拒绝，默认是丢弃。 2.1.2 在 2.1.1 的基础判断上，如果线程池仍然是 RUNNING 的状态，但是 workerCountOf() 拿到的 worker 是 0，那么添加一个 worker，这里只在线程池里面增加一个线程。这里我们可以看到核心线程数满了之后，先添加到队列，如果线程池中的 worker 是 0 的话，那么会新加一个线程，核心线程会带着任务直接执行，而核心线程之外的线程是从队列里面取任务来执行的，注意 addWorker() 方法的调用。 2.2 线程池状态并不是 RUNNING 状态的话，或者任务进入队列失败了，尝试创建worker执行任务，实际上 addWorker() 方法里面也是判断了线程池状态的，不是 RUNNING 状态的话直接返回 false，添加任务失败，触发 reject 策略。 addWorker 分析在上面添加任务的分析过程中，主要是调用 addWorker() 的方法，现在来窥探下 addWorker() 方法： 点击展开：addWorker() 方法 >folded1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class ThreadPoolExecutor extends AbstractExecutorService { private boolean addWorker(Runnable firstTask, boolean core) { retry: for (int c = ctl.get(); ; ) { /** * 这里的 if 条件等价于 * if(rs &gt; SHUTDOWN || * rs == SHUTDOWN &amp;&amp; firstTask != null || * rs == SHUTDOWN &amp;&amp; workQueue.isEmpty() * ) * 1. 线程池状态 &gt; SHUTDOWN 时，直接返回false； * 2. 线程池状态 = SHUTDOWN 时，且 firstTask 不等于 null，直接返回false； * 3. 线程池状态 = SHUTDOWN 时，且工作队列是空的话，直接返回false； * */ // Check if queue empty only if necessary. if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) return false; // 内存自旋 for (; ; ) { // worker 超过容量，直接返回 false if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; // 使用 CAS 的方式增加 worker 数量，成功了跳出外层 for 循环 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl // 如果线程状态发生变化外层自旋。 if (runStateAtLeast(c, SHUTDOWN)) continue retry; // else CAS failed due to workerCount change; retry inner loop } } boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); // 重新检查线程池的状态 if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) { if (t.getState() != Thread.State.NEW) throw new IllegalThreadStateException(); workers.add(w); workerAdded = true; int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; } } finally { mainLock.unlock(); } // 启动 worker 的线程 if (workerAdded) { t.start(); workerStarted = true; } } } finally { if (!workerStarted) addWorkerFailed(w); } return workerStarted; }} &emsp;&emsp;这个方法的代码分为两部分，第一部分是 for() 循环，也就干了一件事把 workerCount 加 1，第二部分循环后执行的代码，目的是添加一个 Worker 对象。在第一 部分代码里面，主要就是第一层的循环判断当前的线程池状态，如果是大于 shutdown 的话就直接返回 false。第二层的循环，先去判断当前的线程数量是不是超过了 corePoolSize 和 maximumPoolSize，然后再用 CAS 的方式把 workerCount 加 1，这是第一部分代码的主要逻辑。 我们再来看看第二部分代码的逻辑，就是 new 一个 Worker 对象，然后把封装任务信息，添加到 workers 集合里面，因为涉及到多线程，肯定需要加锁的，这里用 ReentrantLock 来实现的，加入之后，启动这个线程，开始执行任务。addWorker() 这个方法的思路还是比较清晰的。 接下来我们来看一看 Worker 这个类： 点击展开：Worker 类 >folded12345678910111213141516171819202122232425262728293031public class ThreadPoolExecutor extends AbstractExecutorService { private final class Worker extends AbstractQueuedSynchronizer implements Runnable { /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /** Delegates main run loop to outer runWorker. */ public void run() { runWorker(this); } }} &emsp;&emsp;主要看一下构造方法，在构造方法里面最重要的就是把当前的 Worker 对象 this 当作参数进行 thread 的生成，实际上等价与 new Thread(runnable)。 因为 Worker 本身是继承了 AQS 类又实现了 Runnable 类的。所以我们要看一下 Worker 重写 run() 方法里面 runWorker() 方法。 runWorker 核心线程执行逻辑先来看一下 runWorker 的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ThreadPoolExecutor extends AbstractExecutorService { final void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; // 为了能够让外部中断 w.unlock(); // allow interrupts // 所有任务完成时标志，线程池正在结束 boolean completedAbruptly = true; try { // 如果 firstTask 不为空就执行 first task，为空就去队列里面取。 while (task != null || (task = getTask()) != null) { w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try { // 空实现，为了扩展 beforeExecute(wt, task); try { task.run(); afterExecute(task, null); } catch (Throwable ex) { afterExecute(task, ex); throw ex; } } finally { // 帮助 GC task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } }} 上面我们提到 Worker 这个类既继承了 AQS 类又实现了 Runnable 类，那么它本身就是一把锁，就可以做同步。很多线程都会去 new Worker()，然后去放自己的任务，所以给自己加了一把锁，然后只执行自己的任务，如果不加锁，别的线程进来之后有可能让它执行其他的任务，所以这个时候要枷锁。这是自己继承 AQS，不用 new其他的锁了，自己 lock 就行了。这一部分实际上就是执行任务，加了很多状态的判断逻辑。 总结来说，首先提交任务 submit (返回 Future 对象) 或者 execute 任务，然后核心线程池够不够，启动核心的，核心线程够了就加入阻塞队列，队列满了，就添加非核心线程，一直到的最大线程数，然后再执行拒绝策略。","link":"/2021-09-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"网络IO模型","text":"背景知识在深入的去了解和学习 IO 模型时，是要求对操作系统的一些底层概念是要了解的，比如 VFS树、FD、Page Cache、Dirty Page、Flush 等。这些概念到时候我会再学习一下，然后单独去写一篇操作系统关键性概念的文章。另外，在学习 IO 的时候，TCP/IP 协议也是需要了解的，包括三次握手以及 linux 中在建立连接时的系统调用信息，这对于理解 IO 很有帮助。 C10K 问题 C10K 问题介绍 线程是 CPU 调度的最小单位，进程是 CPU 分配资源的最小单位 C10K 是一个经典的服务端问题。最初的服务器是基于进程/线程模型，新的一个 TCP 连接，就需要分配一个进程（或者线程），如果有 10k 个连接，那么需要创建 1w 个进程（或线程），对于单机服务器来说，这是无法承受的。所以如何去突破单机的性能，是高性能网络编程必须要面对的问题。关于 C10K 问题的探讨，可以参考这篇文章 The C10K problem. 解决方案从 C10K 的背景中看出，要向解决高并发的连接问题，无非有两种，一是一个连接一个线程，另一个是多个连接一个线程。前者会因为系统资源而限制，就算系统资源充足，实际上效率也并不高，会涉及到大量的线程上下文切换操作，扩展性差，后者是现在的主流处理方式。 同步与异步在涉及到 IO 问题时，通常会聊到同步与异步的概念，所谓的同步异步是在于应用程序读写数据还是内核来读写数据，如果说读操作是应用程序来读（调用 read() 的 system call 方法），写操作是应用程序来写（调用 write() 的 system call 方法），那么这种模型就是同步的；如果是内核来做的读写，那么就是异步的。实际上，在目前的 linux 内核版本（2.6）是没有异步的实现，只有 Windows 实现了异步模型。 阻塞与非阻塞阻塞与非阻塞也是 IO 中经常容易混淆的概念，应用程序在发起读写操作时，对应就是某一个线程发起读写操作，那么会通过系统内核来进行 IO 操作，发起读写操作的线程如果一直等待内核 IO 操作完成以后，才能执行其他的操作，那么这种模型就是阻塞的，如果不需要等待内核 IO 操作完成，可以直接进行后续的操作，此时内核会立即返回给线程一个状态值。那么这种模型就是非阻塞的。 综上所述，IO 模型可以分为四类，同步阻塞、同步非阻塞、异步阻塞、异步非阻塞，实际上异步阻塞 是没有意义的。所以目前 IO 主要分为三大类，而 Linux 只有前两类的实现，只有 Windows 实现了异步非阻塞的模型。接下来再了解一下同步阻塞和同步非阻塞的 IO 模型。 IO 模型 系统环境：CentOS 7.9，内核版本是 2.6JDK 版本：11 BIO（同步阻塞）BIO（Blocking IO） 也是常说的传统 IO，java 中的实现在 java.io 包中。对于 BIO 的模型，从 linux 内核角度来说就是一个 TCP 连接就新建一个 thread 去处理，在很高的并发连接下，对于系统资源的开销还是比较大的。另外对于数据的读写，都必须要等到内核 IO 操作完成以后，在等待的过程中，线程一直占用资源，利用率不高。看一个 java BIO 的例子。 123456789101112131415161718192021222324252627282930313233343536373839404142public class SocketBio { public static void main(String[] args) throws IOException { // 打开 socket server 服务 final ServerSocket server = new ServerSocket(9999); System.out.println(&quot;start server：&quot; + server.getInetAddress().getHostAddress() + &quot;:&quot; + server.getLocalPort()); // 这里 while 是模拟 BIO 模型下一直在等待连接 while (true) { System.out.println(Thread.currentThread().getName() + &quot;: 服务器正在等待连接...&quot;); final Socket client = server.accept(); // 阻塞住 System.out.println(&quot;client: &quot; + client.getInetAddress() + &quot;:&quot; + client.getPort()); // 当客户端连接成功以后，开启一个线程来处理 new Thread(() -&gt; { try { // 拿到字节流，socket 通信 final InputStream in = client.getInputStream(); // 字节流转成字符流 final BufferedReader reader = new BufferedReader(new InputStreamReader(in)); System.out.println(Thread.currentThread().getName() + &quot;: 服务器正在等待数据...&quot;); // 一直循环等待消息 while (true) { final String line = reader.readLine(); if (null != line) { System.out.println(Thread.currentThread().getName() + &quot;: 服务器已经接收到数据：&quot; + line); } else { client.close(); break; } } System.out.println(Thread.currentThread().getName() + &quot;: 客户端 --&gt; &quot; + client.getInetAddress().getHostAddress() + &quot;断开！&quot;); } catch (IOException e) { e.printStackTrace(); } }).start(); } }} 编译后在linux上跑一下（我的linux内核版本是 2.6），用linux工具命令strace 来跟踪该 java 进程的系统调用情况，运行：strace -ff -o out java top.caolizhi.example.io.bio.SocketBio跟踪的日志输出到 out 文件中。服务端开始启动，并且阻塞在 accept 代码段，输出： 我们看到有很多 out.pid 格式的文件产生，如下图所示，最小的 pid 是主进程，还有垃圾回收进程等，应该都是 fork 出来的子进程。 再来看一下服务端打开的TCP端口监听，执行命令netstat -natp可以看到 java 进程的 pid 是 7184，然后当前的状态是 LISTEN 状态 再来看一下 7184 这个进程打开 FD（File Descriptor 文件描述符）有哪些，执行命令：lsof -p 7184lsof命令是查看当前系统文件的工具，一切皆文件，我们查看的是 java 进程打开了哪些文件（FD），如图： 还可以看一下 java 进程 socket 的信息，执行命令：ll /proc/7184/fd看到当前打了一个 fd = 5 的 socket，与 lsof 的结果一致。 看了 java 进程打开的 FD 之后，接来下就看一下 java 进程 7184 的系统调用日志，打开 out.7184，主要看到最后几行的片段： 12345678910stat(&quot;/etc/sysconfig/64bit_strstr_via_64bit_strstr_sse2_unaligned&quot;, 0x7fff49a171a0) = -1 ENOENT (No such file or directory)mprotect(0x7f5abc955000, 806912, PROT_READ) = 0munmap(0x7f5abd8d9000, 28940) = 0mmap(NULL, 1052672, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0x7f5abd7d3000clone(child_stack=0x7f5abd8d2fb0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7f5abd8d39d0, tls=0x7f5abd8d3700, child_tidptr=0x7f5abd8d39d0) = 7185futex(0x7f5abd8d39d0, FUTEX_WAIT, 7185, NULL 实际上，7184 的进程里面又 clone 了一个子进程 7185，我们再去 7185 这个进程里面看一下，在 7185 的进程里面有几个关键的系统调用：bind(),listen() 12345678910bind(5, {sa_family=AF_INET6, sin6_port=htons(9999), inet_pton(AF_INET6, &quot;::&quot;, &amp;sin6_addr), sin6_flowinfo=htonl(0), sin6_scope_id=0}, 28) = 0listen(5, 50) .......write(1, &quot;start server\\357\\274\\2320.0.0.0:9999&quot;, 27) = 27write(1, &quot;\\n&quot;, 1) = 1write(1, &quot;\\346\\234\\215\\345\\212\\241\\345\\231\\250\\346\\255\\243\\345\\234\\250\\347\\255\\211\\345\\276\\205\\350\\277\\236\\346\\216\\245...&quot;, 30) = 30write(1, &quot;\\n&quot;, 1) = 1mprotect(0x7f5ab418d000, 32768, PROT_READ|PROT_WRITE) = 0pread64(3, &quot;\\312\\376\\272\\276\\0\\0\\0007\\0026\\n\\0\\6\\1_\\t\\0\\236\\1`\\t\\0\\236\\1a\\t\\0\\236\\1b\\t\\0&quot;..., 18001, 9986808) = 18001poll([{fd=5, events=POLLIN|POLLERR}], 1, -1 上面操作中，bind()把等于 5 的 FD 和端口 9999 绑定，listen监听 5 这个 FD。然后调用 write() 的系统调用把需要打印信息写到标准输入的FD 1。最后是 poll() 和以前的 accept() 类似，阻塞等待。 实际上这里有点疑问，之前的系统调用是 accept 阻塞等待，但是现在变成了 poll 接下来，我们接入一个客户端，看一下是什么情况，执行命令：nc localhost 9999看到有信息输出： 再来看一下 7184 fd 的情况： 可以看到 fd = 6 的 socket 连接，状态为 ESTABLISHED。当有客户端连接以后，代码里面会新建一个线程来处理，我们得到了 out.7382 的日志： 上图中，大概能看到 thread 的名称，pid 号, 还有系统调用 recvfrom(6,)，等待接收数据。 以上就是一个 BIO 的底层实现流程,从上面的过程可以看到，服务端有两次阻塞，第一次是启动后等待客户端连接(accept)，第二次是在客户端连接后等待客户端发送数据(recevfrom)，如果没有数据，服务会一直阻塞住。另外如果我再开一个客户端连接，那么会新开一个线程去处理，大量的请求连接会造成服务器的压力。 另外我们可以还可以看一下客户端和服务端建立 TCP 连接的三次握手，四次挥手的过程，执行命令：tcpdump -nn -i ens33 port 999过程如下： NIO（同步非阻塞）针对 BIO 的劣势，我们考虑在单线程服务器处理，即我不去新建一个线程去处理，所有的请求在同一个线程里面处理，（其实这里你可以想一下 redis），但是这样会有一个问题就是一个连接请求进来了，线程阻塞住在等待客户端发送数据，如果另一个客户端连接过来，那么服务端无法处理，我们进行优化一下，可以这样去解决，如果等待数据的阻塞，还可以继续接收客户端的连接，再继续优化，如果等待数据时阻塞住了，那么我们遍历下一个 socket client，我们改进一下 BIO 的代码。12345678910111213141516171819202122232425262728293031323334353637383940public class SocketNio { public static void main(String[] args) throws IOException, InterruptedException { List&lt;SocketChannel&gt; clientList = new LinkedList&lt;&gt;(); final ServerSocketChannel socketChannel = ServerSocketChannel.open(); socketChannel.bind(new InetSocketAddress(9999)); socketChannel.configureBlocking(false); // 不设置，就是阻塞，调用 accept()，这里解决 BIO 的第一个阻塞 System.out.println(&quot;server started ...&quot; + socketChannel.socket().getInetAddress() + &quot;:&quot; + socketChannel.socket().getLocalPort()); while (true) { Thread.sleep(1000); final SocketChannel client = socketChannel.accept(); if (null != client) { client.configureBlocking(false); // 配置非阻塞，否则一直等待客户端的数据，这里解决 BIO 的第二个阻塞 System.out.println(&quot;client :&quot; + client.socket().getInetAddress() + &quot;:&quot; + client.socket().getPort()); clientList.add(client); }else { System.out.println(&quot;waiting for connection ....&quot;); } final ByteBuffer byteBuffer = ByteBuffer.allocate(4096); // 遍历客户端，读写数据 for (SocketChannel channel : clientList) { System.out.println(&quot;read data from client &quot; + channel.socket().getInetAddress() + &quot;:&quot; + channel.socket().getPort()); final int byteNum = channel.read(byteBuffer); if (byteNum &gt; 0) { byteBuffer.flip(); // 翻转，由写转成读 final byte[] readBytes = new byte[byteBuffer.limit()]; byteBuffer.get(readBytes); // 把 buffer 里面的数据 copy 到 readBytes 数组 final String data = new String(readBytes); System.out.println(channel.socket().getInetAddress() + &quot;:&quot; + channel.socket().getPort() + &quot;'s data :&quot; + data) ; } } } }} 上述代码中，server 端和 client 端都设置成非阻塞的方式，configureBlocking(false)，并且将连接放在一个list集合中，在等待客户端消息时，看看消息是否准备好，遍历 list 集合，如果有消息则打印出来。我们可以再来看一下在启动服务的时候，第一次设置成阻塞模式的系统调用，同样是执行：strace -ff -o out java top.caolizhi.example.io.nio.SocketNio如下图，可以看到 server 端阻塞住了： out.pid 找到 java 进程 fork 出来的监听子进程，1463，查看 1463 的 socket fd，如下： 再来看一下系统调用： 1234567891011write(1, &quot;server started .../0:0:0:0:0:0:0&quot;..., 39) = 39write(1, &quot;\\n&quot;, 1) = 1futex(0x7f31a4026f54, FUTEX_WAIT_BITSET_PRIVATE, 1, {tv_sec=1884, tv_nsec=45138802}, 0xffffffff) = -1 ETIMEDOUT (Connection timed out)futex(0x7f31a4026f28, FUTEX_WAKE_PRIVATE, 1) = 0pread64(3, &quot;\\312\\376\\272\\276\\0\\0\\0007\\0:\\n\\0\\r\\0%\\n\\0&amp;\\0'\\t\\0\\f\\0(\\7\\0)\\7\\0*\\n&quot;..., 1072, 5183108) = 1072pread64(3, &quot;\\312\\376\\272\\276\\0\\0\\0007\\0005\\t\\0\\10\\0!\\n\\0\\t\\0\\&quot;\\t\\0\\36\\0#\\t\\0\\36\\0$\\t\\0&quot;..., 1161, 10456208) = 1161pread64(3, &quot;\\312\\376\\272\\276\\0\\0\\0007\\0\\t\\7\\0\\7\\7\\0\\10\\1\\0\\tinterrupt\\1\\0\\25(&quot;..., 162, 17327013) = 162pread64(3, &quot;\\312\\376\\272\\276\\0\\0\\0007\\0\\35\\n\\0\\5\\0\\25\\n\\0\\26\\0\\27\\n\\0\\4\\0\\30\\7\\0\\31\\7\\0\\32\\1&quot;..., 460, 17355571) = 460mprotect(0x7f31a419b000, 4096, PROT_READ|PROT_WRITE) = 0rt_sigaction(SIGRT_30, {sa_handler=0x7f318aa21480, sa_mask=[], sa_flags=SA_RESTORER, sa_restorer=0x7f31aa65c630}, {sa_handler=0x7f318a810020, sa_mask=[], sa_flags=SA_RESTORER, sa_restorer=0x7f31aa65c630}, 8) = 0accept(4, 看，看到了吧，调用 accept(4,) 一直在监听 fd = 4 ，等待客户端连接，我们看一下 accept 的方法： 1int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);; accept 系统调用目的就是等待一个 socket connection，这也印证了 BIO 的模型。把 server 端和 client 端全部设置成非阻塞模式，运行： 一直在循环等待 client 连接，现在打开一个 client 去连接server，同样执行nc 127.0.0.1 9999 再来发送一组数据，比如”test nio”，我们可以看到 server 端接收到了数据打印了出来： 再开启一个客户端来连接服务端，发送数据”client2 test”，服务端如下图： 最后看一下 NIO 的系统调用片段： 123socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 3 .....accept(4, 0x7f1fe2065730, [28]) = -1 EAGAIN (Resource temporarily unavailable) 可以看到 socket 调用使用 SOCK_NONBLOCK 方式，另外，每一次调用 accept() 都会返回一个状态码，-1 表示没有连接可用。用man 2 socket来看一下描述： 综上所述，在非阻塞模式下，解决了 BIO 的两次阻塞，但是非阻塞也有弊端，我每一次都要循环遍历所有的 socket 连接，才能看到是否有消息发送，如果有大量的连接的话，显然效率是极低的。况且并不是所有的连接都是有消息的，每次仍然轮询的那些连接，也是不合理的。 那么，NIO 中，java 程序一直在轮询所有的连接，不断地在用户态和内核态切换，如果，把轮询放到内核中去做，那岂不是效率要高的多，这就引出来多路复用的模式。 多路复用（同步非阻塞）首先理解一下多路复用的概念，”多路” 实际上指的就是多个 IO，多个 socket 连接，也就是说单个线程通过记录跟踪每一个 IO 的状态，来同时管理多个 IO。IO 多路复用的实现主要有三种，按照出现的时间顺序为：select，poll，epoll。 selectselect 可以传入一个 fd 数组，内核需要开辟空间来存这部分 fd，然后去轮询，就算有数据也只是修改状态，然后全部返回给应用，并不会告诉应用那些 IO 是有数据的，所以应用还是要轮询一次，找有变化的 IO，再调用 read 取读取数据。所以这样就会有 2 次轮询和 2 次数据拷贝，另外 select 只支持最大 1024 个 fd，另外 select 是线程不安全的。 poll其实 poll 跟 select 差不多，但是可以支持任意个 fd，没有 1024 大小的限制，但是会受到系统文件描述符的限制，可用命令 ulimit -a 查看系统的限制。可以看下 POLL 方法描述 epollepoll 是最新的 IO 多路复用的实现，linux 内核 2.6 以后才出现。epoll 做了两件事情，第一件事就是，在内核中，使用红黑树来维护所有的需要检查的 fd，红黑树的时间复杂度是 O(logN)，另外一件事就是使用了事件驱动机制，在内核中维护了一张链表，把有状态的 fd 都放到那个链表里面，应用直接来取有状态的 fd 集合，效率大大的提高。epoll(7) 的方法调用又分为三个系统调用，epoll_create,epoll_ctl,epoll_wait。 应用调用 epoll_create 方法会在内核开辟一个红黑树结构，返回一个 fd 应用调用 epoll_ctl 方法传入需要监听的 fd 和针对该 fd 相应的事件，是读还是写等？那么内核会不断地轮询该 fd，看是否有状态变化 应用程序 epoll_wait 方法，内核会返回一个链表，链表里面是有状态变化的 fd，应用程序再去遍历这些 fd，调用 read() 或 write() 进行操作 链表里面的 fd 是从中断处理那边产生的，网卡中断会把 fd 放到 fd buffer 里面，然后内核再把 fd buffer 里面的 fd 添加到红黑树，接着把有状态变化的 fd 放到链表里面。 接下来，再来看一个 java 多路复用的例子，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class SocketMultiplexing { public static void main(String[] args) throws IOException { // 开启一个服务端 final ServerSocketChannel server = ServerSocketChannel.open(); server.configureBlocking(false); server.bind(new InetSocketAddress(9999)); // Selector 类是多路复用的 java 实现 final Selector selector = Selector.open(); // 相当于调用了 epoll_create /** * 如果是 epoll 模型：相当于调用了 epoll_ctl，监听 EPOLLIN 事件 * 如果是 select/poll 模型：会在 jvm 里面开辟一个数组，把 fd 放进去。 */ server.register(selector, SelectionKey.OP_ACCEPT); System.out.println(&quot;server start: &quot; + server.socket().getInetAddress() + &quot;:&quot; + server.getLocalAddress()); while (true) { final Set&lt;SelectionKey&gt; selectionKeys = selector.keys(); System.out.println(&quot;total checking fd size: &quot; + selectionKeys.size()); if (selector.select() &gt; 0 ) { // select() 方法就是拿到有 IO 状态变化的 fd 数量 final Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); // 拿到 IO 状态变化的 fd 集合 final Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator(); while (keyIterator.hasNext()) { final SelectionKey key = keyIterator.next(); // 拿到 fd keyIterator.remove(); // 移除掉，不然一直在循环处理 // 接下来，对不同的 IO 事件进行处理，是建立连接还是读数据还是写数据？ if (key.isAcceptable()) { final ServerSocketChannel channel = (ServerSocketChannel)key.channel(); // 拿到的是 ServerSocketChannel，服务端 final SocketChannel client = channel.accept(); // accept 之后会拿到一个新的 fd client.configureBlocking(false); System.out.println(&quot;client: &quot; + client.socket().getInetAddress() + &quot;:&quot; + client.getLocalAddress()); ByteBuffer buffer = ByteBuffer.allocate(4096); //需要把上面调用 accept 产生的新的 fd 也要放到监听的列表里面去，并且监听的时间是 READ，绑定一个 buffer 到这个 fd 上。 client.register(selector, SelectionKey.OP_READ, buffer); } else if (key.isReadable()) { final SocketChannel client = (SocketChannel) key.channel(); // 拿到的是 SocketChannel 对象，客户端 // 拿到客户端传过来的数据，一个 buffer，因为上面 register 的时候，绑定了一个 buffer 在这个 fd 上。 final ByteBuffer buffer = (ByteBuffer) key.attachment(); buffer.clear(); while (true) { final int read = client.read(buffer); if (read &gt; 0) { // 有数据 buffer.flip(); // 翻转，由读变成写 while (buffer.hasRemaining()) { client.write(buffer); // 写回 client } } else if (read == 0) { // 没有数据 break; } else { client.close(); break; } } } } } } }} 上面的例子根据注释能够看懂做了什么事情，不再详述，运行该程序：strace -ff -o out java top.caolizhi.example.io.multiplexing.SocketMultiplexing 一开始服务启动会得到一个监听的 fd，所以 total checking fd size 是 1。 根据 out.pid 文件看一下当前 java 进程的 fd，ll /proc/1931/fd可以看到一个类型为 eventpoll 的 fd 5，还有两个 pipe 类型的 fd， 再用 lsof 命令执行一下： 可以看到程序打开了一个类型为 eventpoll 的 fd 5 ，还有只能 write 的管道 fd 6以及只能 read 的管道 fd 7。我们添加一个客户端来连接服务器，同样用 nc 命令，执行完后： 可以看到，当有客户端连接后，产生一个新的 socket fd，程序把这个新的 socket fd 调用 register 方法注册 READ 事件，那么此时total checking fd size 是 2。然后在客户端发送数据，”abc“,”dddd“，服务端也会回写同样的数据给客户端。如图： 接下来整体看一下这个程序的系统调用追踪： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 建一个 socket io，返回fd 4socket(AF_INET6, SOCK_STREAM, IPPROTO_IP) = 4# 绑定端口bind(4, {sa_family=AF_INET6, sin6_port=htons(9999), inet_pton(AF_INET6, &quot;::&quot;, &amp;sin6_addr), sin6_flowinfo=htonl(0), sin6_scope_id=0}, 28) = 0# 监听 socket 连接listen(4, 50) = 0 ...........# 创建一个 epoll 实例，返回fd 5epoll_create(256) = 5# 创建管道pipe([6, 7]) = 0fcntl(6, F_GETFL) = 0 (flags O_RDONLY)fcntl(6, F_SETFL, O_RDONLY|O_NONBLOCK) = 0fcntl(7, F_GETFL) = 0x1 (flags O_WRONLY)fcntl(7, F_SETFL, O_WRONLY|O_NONBLOCK) = 0# 针对 5 的 epoll 实例，添加一个监听 fd 6，监听的时间是 EPOLLINepoll_ctl(5, EPOLL_CTL_ADD, 6, {EPOLLIN, {u32=6, u64=140041703653382}}) = 0# 调用系统 write 方法，打印信息write(1, &quot;server start: /0:0:0:0:0:0:0:0:/&quot;..., 52) = 52 ........... write(1, &quot;total checking fd size: 1&quot;, 25) = 25# 针对 5 的 epoll 实例，添加一个监听 fd 4，监听的时间是 EPOLLINepoll_ctl(5, EPOLL_CTL_ADD, 4, {EPOLLIN, {u32=4, u64=140041703653380}}) = 0 ........... # 返回一个有 IO 状态改变的 fd 数量，因为此时发生了客户端的连接，有事件产生epoll_wait(5, [{EPOLLIN, {u32=4, u64=140041703653380}}], 1024, -1) = 1# 接受了一个客户端连接，返回fd 9accept(4, {sa_family=AF_INET6, sin6_port=htons(59298), inet_pton(AF_INET6, &quot;::ffff:192.168.170.112&quot;, &amp;sin6_addr), sin6_flowinfo=htonl(0), sin6_scope_id=0}, [28]) = 9fcntl(9, F_GETFL) = 0x2 (flags O_RDWR) ...........write(1, &quot;client: /192.168.170.112:/192.16&quot;..., 46) = 46write(1, &quot;\\n&quot;, 1) = 1write(1, &quot;total checking fd size: 2&quot;, 25) = 25write(1, &quot;\\n&quot;, 1) = 1# 把 fd 9 添加到监听列表中epoll_ctl(5, EPOLL_CTL_ADD, 9, {EPOLLIN, {u32=9, u64=140041703653385}}) = 0epoll_wait(5, [{EPOLLIN, {u32=9, u64=140041703653385}}], 1024, -1) = 1 ...........# 读取 fd 9 的 socket 数据read(9, &quot;abc\\n&quot;, 4096) = 4pread64(3, &quot;\\312\\376\\272\\276\\0\\0\\0007\\0.\\n\\0\\t\\0+\\7\\0,\\5\\377\\377\\377\\377\\377\\377\\377\\376\\5\\377\\377\\377\\377&quot;..., 1021, 17297719) = 1021# 把 fd 9 socket 读取到数据再写回到 fd 9 的通道中write(9, &quot;abc\\n&quot;, 4) = 4write(1, &quot;total checking fd size: 2&quot;, 25) = 25futex(0x7f5ea80bdb54, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x7f5ea80bdb50, FUTEX_OP_SET&lt;&lt;28|0&lt;&lt;12|FUTEX_OP_CMP_GT&lt;&lt;24|0x1) = 1write(1, &quot;\\n&quot;, 1) = 1epoll_wait(5, [{EPOLLIN, {u32=9, u64=140041703653385}}], 1024, -1) = 1read(9, &quot;dddd\\n&quot;, 4096) = 5futex(0x7f5ea80bf954, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x7f5ea80bf950, FUTEX_OP_SET&lt;&lt;28|0&lt;&lt;12|FUTEX_OP_CMP_GT&lt;&lt;24|0x1) = 1write(9, &quot;dddd\\n&quot;, 5) = 5write(1, &quot;total checking fd size: 2&quot;, 25) = 25write(1, &quot;\\n&quot;, 1) = 1# 没有任何 IO 状态发生变量epoll_wait(5, 另外，根据 epoll 的系统调用文档，又分为边缘触发和水平触发，这里不拓展了，有兴趣可以去看 man page 文档。 AIO（异步非阻塞）linux 内核是没有实现 AIO 的。它与同步非阻塞的区别在于，不需要一个线程去轮询 IO 的状态改变，而是一个 IO 的状态变更，系统会通知相应的线程来处理。JDK 中的 AIO 的底层实现也是基于 epoll 来实现的，并非真正的异步 IO。 参考https://tech.meituan.com/2016/11/04/nio.htmlhttps://notes.shichao.io/unp/ch6/深入理解计算机操作系统","link":"/2021-09-%E7%BD%91%E7%BB%9CIO%E6%A8%A1%E5%9E%8B/"},{"title":"JVM 系列 ———— JVM 基础","text":"写在前面最近想把之前学习 JVM 做的笔记整理一下，想写一个 JVM 的系列，主要包括一下部分： JVM 基础 类文件格式 类加载 java 运行时数据区 对象 JMM GC 基础知识 GC 常用垃圾回收器 GC 调优的问题 目的就是想复习一下知识点，然后巩固一下基础。很多东西不去用不去深入探究就会忘记。所以还是想再记录一下。 java 从编码到执行首先，一个 java 文件从编码到运行的流程是一个怎样的流程，在 java 编程里面，我们最常用的命令就是 java 和 javac，我们会先执行 javac 编译 java 文件，通过编译器翻译成 java 字节码文件即 .class 文件，然后我们再通过 java 命令就可以让某个程序跑起来。所以这两个命令就代表了从编码到执行的两个过程，如下图：","link":"/2021-10-JVM%E7%B3%BB%E5%88%97-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"title":"K8S的label和annotation区别","text":"概述label 和 annotation 都可以将元数据关联到 Kubernetes 的资源对象。与资源对象相关的事务与动作。label 主要用于来选择对象，可以指定满足特定条件的对象。但是 annotation 不能标识和选择对象。annotation 中的元数据可以是结构化或者非结构化的，也可以包含 label 中不允许出现的字符。 资源对象的名称 ( Name ) 、标签、注解这三个属性属于资源对象的元数据(metadata) 数据结构两者都是 key-value 键值对的形式： 123annotations: key1: value1 key2: value2 123labels: key1: value1 key2: value2 labellabel 是附着到资源对象上（例如 Pod ）的键值对。可以在创建资源对象的时候指定，也可以在资源对象创建后随时指定。labels 的值对系统本身并没有什么含义，只是对用户才有意义。 123labels: key1: value1 key2: value2 Kubernetes 最终将对 labels 最终索引和反向索引用来优化查询和 watch，在 UI 和命令行中会对它们排序。不要在 label 中使用大型、非标识的结构化数据，记录这样的数据应该用 annotation。 一般配合 label selector 使用。 annotationannotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。 123annotations: key1: value1 key2: value2 以下列出了一些可以记录在 annotation 中的对象信息： 声明配置层管理的字段。使用 annotation 关联这类字段可以用于区分以下几种配置来源：客户端或服务器设置的默认值，自动生成的字段或自动生成的 auto-scaling 和 auto-sizing 系统配置的字段。 创建信息、版本信息或镜像信息。例如时间戳、版本号、git 分支、PR 序号、镜像哈希值以及仓库地址。 记录日志、监控、分析或审计存储仓库的指针 可以用于 debug 的客户端（库或工具）信息，例如名称、版本和创建信息。 用户信息，以及工具或系统来源信息、例如来自非 Kubernetes 生态的相关对象的 URL 信息。 轻量级部署工具元数据，例如配置或检查点。 负责人的电话或联系方式，或能找到相关信息的目录条目信息，例如团队网站。 如果不使用 annotation，也可以将以上类型的信息存放在外部数据库或目录中，但这样做不利于创建用于部署、管理、内部检查的共享工具和客户端库。 参考 《Kubernetes权威指南第5版》 https://jimmysong.io/kubernetes-handbook/concepts/annotation.html https://jimmysong.io/kubernetes-handbook/concepts/label.html","link":"/2021-10-K8S%E7%9A%84label%E5%92%8Cannotation%E5%8C%BA%E5%88%AB/"},{"title":"MongoDB GridFS 存储文件","text":"前言在开发中，会碰到存储图片或者视频的问题，也就是大文件存储的问题，能想到的有三种解决方式： 本地文件系统 数据库，比如 MongoDB 云存储，比如 Amazon S3 其他两种都比较好理解，现在想来看一下 MongoDB 是怎么来存储图片或者视频的。看到官网的文档（版本是 5.0）是如果文件大小 &lt; 16MB，一个文档是可以存的下的，超过 16MB 的话需要用 GridFS。 关于 GridFSGridFS 是 MongoDB 中存储超过 16M 文件的存储方式和约定，实现的思路就是分而治之。GridFS 默认的块（chunk）大小为 256kB，用两个 collection（chunks） 去存文件，一个 collection（files） 来存储切分的 chunk，另外一个 collection 来存储文件的元数据。这两个 collection 都在同一个 bucket 名为 fs 下。 什么时候用 GridFS在 MongoDB 中，虽然使用 GridFS 来存储超过 16MB 大小的文件，但是有些情况下，用 MongoDB 来存储文可能比本地文件系统来存储有效的多。官方文档给了下面三种情况： 本地文件系统有文件或者目录的限制，GridFS 没有限制 不需要拿整个文件加载到内存就可以取到文件的所需要的部分数据，GridFS 可以做到。 文件自动同步，尤其在分布式的应用上。 怎么使用 GridFS基于 spring boot 搭建的示例项目来展示一下: 配置 123456789# 配置上传单个文件大小无限制spring.servlet.multipart.max-file-size=-1# 配置上传总大小无限制spring.servlet.multipart.max-request-size=-1# MongoDB 的配置spring.data.mongodb.uri=mongodb://root:root123@127.0.0.1:27017spring.data.mongodb.database=testspring.data.mongodb.gridfs.database=uploads thymeleaf 模板页面代码 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;html xmlns:th=&quot;https://www.thymeleaf.org&quot;&gt;&lt;body&gt;&lt;h1&gt;&lt;font color=&quot;blue&quot;&gt;上传文件到 MongoDB &lt;/font&gt; &lt;/h1&gt;&lt;div th:if=&quot;${message}&quot;&gt; &lt;h2 th:text=&quot;${message}&quot;/&gt;&lt;/div&gt;&lt;div&gt; &lt;form method=&quot;POST&quot; enctype=&quot;multipart/form-data&quot; action=&quot;/gridFs/upload&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;File to upload:&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;file&quot; name=&quot;file&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;submit&quot; value=&quot;Upload&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;form method=&quot;POST&quot; action=&quot;/gridFs/delete&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;submit&quot; value=&quot;deleteAll&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;&lt;div&gt; &lt;ul&gt; &lt;li th:each=&quot;file : ${files}&quot;&gt; &lt;a th:href=&quot;@{'http://127.0.0.1:8080/gridFs/' + ${file} + '/download'}&quot; th:text=&quot;${file}&quot;/&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; Controller 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Controller@RequestMapping(&quot;/gridFs&quot;)public class GridFsController { private final GridFsService gridFsService; @Autowired public GridFsController(GridFsService gridFsService) { this.gridFsService = gridFsService; } // 主页 @GetMapping(&quot;/&quot;) public String listUploadFiles(Model model) { model.addAttribute(&quot;files&quot;, gridFsService.loadAllFiles()); return &quot;gridFsForm&quot;; } // 上传接口 @PostMapping(&quot;/upload&quot;) public String handleFileUpload(@RequestParam(&quot;file&quot;) MultipartFile file, RedirectAttributes redirectAttributes) { final long start = System.currentTimeMillis(); gridFsService.upload(file, &quot;user&quot; + new Random().nextInt(100)); final long end = System.currentTimeMillis(); final long period = end - start; final long t = period / 1000; redirectAttributes.addFlashAttribute(&quot;message&quot;, &quot;You successfully uploaded &quot; + file.getOriginalFilename() + &quot;!&quot; + &quot; spent time : &quot; + ( t &lt; 1 ? period + &quot;ms&quot; : t + &quot;s&quot;)); return &quot;redirect:/gridFs/&quot;; } // 下载接口 @GetMapping(&quot;/{filename}/download&quot;) @ResponseBody public ResponseEntity&lt;Resource&gt; serverFile(@PathVariable String filename) { Resource file = gridFsService.download(filename); return ResponseEntity.ok() .header(HttpHeaders.CONTENT_DISPOSITION, &quot;attachment; filename=\\&quot;&quot; + URLEncoder.encode(file.getFilename(), StandardCharsets.UTF_8) + &quot;\\&quot;&quot;) .body(file); } // 删除所有的文件接口 @PostMapping(&quot;/delete&quot;) public String deleteAllFiles(RedirectAttributes redirectAttributes) { gridFsService.deleteAll(); redirectAttributes.addFlashAttribute(&quot;message&quot;, &quot;You successfully deleted all files !&quot;); return &quot;redirect:/gridFs/&quot;; }} Service 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243@Componentpublic class GridFsServiceImpl implements GridFsService { @Autowired GridFsTemplate gridFsTemplate; @Override public void upload(MultipartFile file, String name) { final String fileName = file.getName(); System.out.println(fileName); final BasicDBObject basicDBObject = new BasicDBObject(); basicDBObject.put(&quot;user&quot;, name); ObjectId objectId = null; try { objectId = gridFsTemplate.store(file.getInputStream(), file.getOriginalFilename(), file.getContentType(), basicDBObject); } catch (IOException e) { e.printStackTrace(); } System.out.println(&quot;object id:&quot; + objectId.toString()); } @Override public List&lt;String&gt; loadAllFiles() { List&lt;GridFSFile&gt; gridFSFiles = new ArrayList&lt;&gt;(); gridFsTemplate.find(new Query()).into(gridFSFiles); final List&lt;String&gt; files = gridFSFiles.stream().map(GridFSFile::getFilename).collect(Collectors.toList()); return files; } @Override public Resource download(String fileName) { final GridFsResource[] resources = gridFsTemplate.getResources(fileName + &quot;*&quot;); return resources[0]; } @Override public void deleteAll() { gridFsTemplate.delete(new Query()); }} 演示","link":"/2021-10-MongoDB%E5%AD%98%E5%82%A8%E6%96%87%E4%BB%B6/"},{"title":"VMware PRO 16 复制虚拟机","text":"对于在本地快速搭建一套分布式节点，无须重新从头开始创建虚拟机，可以通过 VMware WorkStation 快照的方式来快速创建一台机器。 单机 node01 ，右键，选择“快照”——&gt; “拍摄快照” 单机 node01 ，右键，选择“管理”——&gt; “克隆”，点击“下一页” “克隆源”页面，选择“现有快照（仅限关闭的虚拟机）”，选择刚才创建的“basic”，点击“下一页” “克隆类型”页面，选择“创建链接克隆”，点击“下一页” “新虚拟机名称”页面，自定义虚拟机名称和存储位置，点击“完成” 等待克隆完成，左侧“我的计算机”列表里面会出现克隆的虚拟机，这里是“node02” 网络配置，因为网络配置的静态 IP，所以需要配置 IP 地址，步骤跟创建虚拟机一样 修改静态IP123cd /etc/sysconfig/network-scriptsvi ifcfg-ens33 只需要修改 IPADDR = 192.168.170.112 即可 修改 hostname修改 /etc/hosts 和 /etc/sysconfig/network CentOS 7.8 如果两个文件已经不能生效，请用下面的命令修改hostnamectl set-hostname {your-custom-hostname} 重启网络服务service restart network执行 ping www.baidu.com 测试，网络畅通","link":"/2021-10-VMwarePro16%E5%A4%8D%E5%88%B6%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"title":"VMware PRO 16 安装虚拟机-自定义分区","text":"通常我们都是按照经典的方式安装虚拟机，由 VMware 自动帮我们分区，没有必要，我们可以选择自定义分区的方式去安装虚拟机，更加充分的利用空间。 在安装完 VMware Workstation 以后，打开页面如下： 点击“创建新的虚拟机”，选择“自定义(高级)(C)”选项，选择“下一步” “选择虚拟机硬件兼容性”页面，默认设置，继续“下一步” “安装客户机操作系统”页面，选择“稍后安装操作系统”，点击“下一步” “选择客户机操作系统”页面，选择“Linux(L)”，版本选择“CentOS 7 64位”，需要根据已经下载好的 iso 文件版本来选择， 点击“下一步” “命名虚拟机”页面，命名虚拟机名字，会在左侧“我的计算机”列表中显示该名称。修改虚拟机存放的位置，可以默认。 如果C盘是固态硬盘且空间足够，可以放在C盘，加快速度。然后点击“下一步” “处理器配置”页面，处理器数量选择“1”就行了，默认值，然后点击“下一步” “此虚拟机的内存” 页面，内存设置成 1G就行了，点击“下一步” “网络类型”页面，默认选择“使用网络地址转换(NAT)(E)”，点击“下一步” “选择I/O控制器类型”页面，默认，点击“下一步” “选择磁盘类型”页面，默认，点击“下一步” “选择磁盘”页面，默认，点击“下一步” “指定磁盘容量”页面，设定最大磁盘大小为30GB，可以设置成更大的值，这里的值并不是一下就分配30GB空间，而是用多少分配多少，直到最大的值。选择“将虚拟磁盘拆分成多个文件”，点击“下一步” “指定磁盘文件”页面，默认，点击“下一步” “已准备好创建虚拟机”页面，点击“自定义硬件”，弹出“硬件”配置窗口 “硬件”页面，左侧选择“新CD/DVD（IDE）”选项， 右侧选择“使用ISO映像文件”，选择本地的 ISO 文件，点击“关闭” 最后点击“完成”，左侧我的计算机列表里会出现“node01”虚拟机 点击“开启此虚拟机”，进入安装页面 选择语言“English”，点击“Continue” 设置“NETWORK &amp; HOST NAME”,定义hostname，这里取“node01”，网络配置，后面再配，这里先点击“apply”，然后点击“Done” 设置“INSTALLATION DESTINATION”，选择“I wil configurepartitioning”，点击“Done”，进入到磁盘分区页面，添加一个分区，选择“/boot”，该区是引导程序的安装目录，分配“512mb”大小，点击“Addmount point” 选中 “/boot”，设置分区格式， “Device Type”选择 “StandardPartition”，“File System”选择“ext4” 添加一个分区，选择“swap”，该区是内存和磁盘的交换空间，分配“2GB”大小，点击“Add mount point”，“swap”分区的格式，默认即可 添加一个分区，选择根目录“/”，“DesiredCapacity“填“max”，即剩下的所有空间都分配给根目录，点击“Add mountpoint“。“Device Type”选择 “LVM”，“File System”选择“ext4” 选中 “/boot”，设置分区格式，“Device Type”选择 “LVM”，“FileSystem”选择“ext4” 分区设置好以后，点击左上角“Done”，弹出“SUMMARY OFCHANGES”框，点击“Accept Changes” 分区完成以后，点击“Begin Installation”，就会进入安装过程。 设置 root 的密码，等待系统安装完成。 安装完成后，点击“Reboot” 重启后，用 root 用户登录，输入密码即可，此时，CentOS 系统已经安装完毕。但是网络不通。 配置网络 [root@node01 \\~]\\# cd /etc/sysconfig/network-scripts/ 找到 ifcfg-ensXX， [root@node01 network-scripts]\\# vi ifcfg-ens33 修改属性： 属性 修改前值 修改后值ONBOOT no yesBOOTPROTO dhcp static 增加属性： 属性 值IPADDR 192.168.170.111NETMASK 255.255.255.0GATEWAY 192.168.170.2DNS1 114.114.114.114 192.168.170.xxx前三段来源于 ： VMware Workstation 菜单“编辑”–&gt; “虚拟网络编辑器(N)”选项， VMnet8 子网地址的前三段，“xxx”段自定义，从0~255任意，但是0,1,255 是保留段，不要用。 192.168.170.2中170 来源同上，其他保持一致。 修改完成后，需要重新启动网络服务，执行命令: service network restart ping 一下百度，可以看到能够联网了。 关闭防火墙 service iptables stop 设置防火墙开机不启动 如果遇到问题：Ncat: No route to host. 防火墙的问题 1:查看防火状态 systemctl status firewalld service iptables status 2:暂时关闭防火墙 systemctl stop firewalld service iptables stop 3:永久关闭防火墙 systemctl disable firewalld chkconfig iptables off 4:重启防火墙 systemctl enable firewalld service iptables restart","link":"/2021-10-VMwarePro16%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"title":"VMware PRO 16 配置共享文件夹","text":"安装 open-vm-tools（CentOS 为例）yum install open-vm-tools open-vm-tools-desktop 设置共享文件夹 查看共享文件夹 vmware-hgfsclient 挂载共享文件夹 sudo vmhgfs-fuse .host:/shared_file /mnt/hgfs其中： shared_file 就是上图中选择的文件夹名称，git。以上图为例，此处我应该将 your_shared_file 替换为 Linux /mnt/hgfs 是挂载点，安装好 open-vm-tools 会自动生成此目录文件。 比如我的虚拟机上： vmhgfs-fuse .host:/caolizhi-personal /mnt/hgfs 设置开机自动挂载 每次关机后，就需要重新挂载。可在文件 /etc/fstab 中添加如下一行：.host:/your_shared_file /mnt/hgfs fuse.vmhgfs-fuse defaults 0 0 比如我的虚拟机上： .host:/caolizhi-personal /mnt/hgfs fuse.vmhgfs-fuse defaults 0 0","link":"/2021-10-VMwarePro16%E9%85%8D%E7%BD%AE%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9/"},{"title":"使用Kubeadm工具快速安装K8S集群","text":"1. 预备工作 准备两台 CentOS 7.8 的虚拟机 安装 docker engineer 清理docker，没有安装过 docker ，没有必要执行 `sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine` 安装 yum-utils，提供了组件 yum-config-manager sudo yum install -y yum-utils 添加 yum 源sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 安装 docker enginesudo yum install docker-ce docker-ce-cli containerd.io 开启 docker 服务sudo systemctl start docker automaticly startupsudo systemctl enable docker 关闭防火墙 12systemctl disable firewalld systemctl stop firewalld 可以配置防火墙的端口号，不用关闭防火墙，各个组件的端口号 组件名称 默认端口 API Server 8080（HTTP 非安全端口号）6443 （HTTPS 安全端口号） Controller Manager 10252 Scheduler 10251 kubelet 1025010255（只读端口号） etcd 2379（供客户端访问）2380 （供 etcd 集群内部节点之间访问） 集群 DNS 服务 53（UPD）53（TCP） 建议在主机上禁用 SELinux修改 /etc/sysconfig/selinux，将 SELINUX=enforcing 修改为 disabled，让容器可以读取主机文件系统 2. 安装 kubeadm 配置 k8s 的 官方yum 源，修改 /etc/yum.repos.d/kubernetes.repo, 内容如下： 12345678[kubernetes] name=Kubernetes Repository name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearchenabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl 运行 yum install 命令安装 kubeadm，kubelet 和 kubectl: yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 启动 kubeadm 12systemctl start kubeletsystemctl enable kubelet kubeadm 需要关闭 linux 的 swap 系统交换区 swapoff -a 3. 修改 kubeadm 的默认配置查看初始化的配置，也称为控制平面（Control Plane）配置和加入节点的配置， kubeadm config print init-defaults 查看 kubeadm init 命令默认参数的内容 init default 配置 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock imagePullPolicy: IfNotPresent name: node taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: 1.22.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} kubeadm config print join-defaults 查看 kubeadm join 命令默认参数的内容 join defaults 配置 apiVersion: kubeadm.k8s.io/v1beta3 caCertPath: /etc/kubernetes/pki/ca.crt discovery: bootstrapToken: apiServerEndpoint: kube-apiserver:6443 token: abcdef.0123456789abcdef unsafeSkipCAVerification: true timeout: 5m0s tlsBootstrapToken: abcdef.0123456789abcdef kind: JoinConfiguration nodeRegistration: criSocket: /var/run/dockershim.sock imagePullPolicy: IfNotPresent name: node01 taints: null 目前暂用默认配置 4. 下载 Kubernetes 的相关镜像需要预先下载所需要的镜像，通过命令 kubeadm config images list 查看 kubeadm config images list 1234567k8s.gcr.io/kube-apiserver:v1.22.3k8s.gcr.io/kube-controller-manager:v1.22.3k8s.gcr.io/kube-scheduler:v1.22.3k8s.gcr.io/kube-proxy:v1.22.3k8s.gcr.io/pause:3.5k8s.gcr.io/etcd:3.5.0-0k8s.gcr.io/coredns/coredns:v1.8.4 执行命令：kubeadm config images pull，如果配置了 config 文件就加上参数，--config=/path/config.yaml 5. 运行 kubeadm init 命令安装 Master 节点首先执行预检查命令，kubeadm init phase reflight 确保主机环境符合安装要求，然后再通过命令 kubeadm init 命令安装 K8S 的 Master 节点。 要说明的一点，K8S 默认设置 cgroup 驱动（cgroupdriver）为 systemd， 而 docker 服务的 cgroup 驱动默认值为 cgroupfs，建议修改为 systemd。修改 Docker 服务的配置文件（默认的位置：/etc/docker/daemon.json），如果没有的话，就新建一个 daemon.json，添加一下配置： 123{ &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;]} 执行预检查命令显示： 123456[preflight] Running pre-flight checks [WARNING Hostname]: hostname &quot;node01&quot; could not be reached [WARNING Hostname]: hostname &quot;node01&quot;: lookup node01 on 114.114.114.114:53: no such host error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2 [ERROR Mem]: the system RAM (972 MB) is less than the minimum 1700 MB [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 上面 error 错误中，前两个 error 原因主要是虚拟机的资源不够，加大到 2 核心 和 2G 内存，最后一个 error 意思是使用了 IPV6 的地址，并且 /proc/sys/net/bridge/bridge-nf-call-iptables 值不为1，通过 echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables 即可。 再次执行预检查命令，只有 warn ，没有 error 了，执行初始化命令。启动失败： 123456789101112131415161718192021222324252627282930313233343536[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s[kubelet-check] Initial timeout of 40s passed.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused. Unfortunately, an error has occurred: timed out waiting for the condition This error is likely caused by: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands: - 'systemctl status kubelet' - 'journalctl -xeu kubelet' Additionally, a control plane component may have crashed or exited when started by the container runtime. To troubleshoot, list all containers using your preferred container runtimes CLI. Here is one example how you may list all Kubernetes containers running in docker: - 'docker ps -a | grep kube | grep -v pause' Once you have found the failing container, you can inspect its logs with: - 'docker logs CONTAINERID'error execution phase wait-control-plane: couldn't initialize a Kubernetes clusterTo see the stack trace of this error execute with --v=5 or higher kubelet 没有启动起来，查看日志：tail /var/log/messages 12345678910Nov 1 15:04:51 node01 kubelet: I1101 15:04:51.235877 8809 docker_service.go:242] &quot;Hairpin mode is set&quot; hairpinMode=hairpin-vethNov 1 15:04:51 node01 kubelet: I1101 15:04:51.235947 8809 cni.go:239] &quot;Unable to update cni config&quot; err=&quot;no networks found in /etc/cni/net.d&quot;Nov 1 15:04:51 node01 kubelet: I1101 15:04:51.242254 8809 cni.go:239] &quot;Unable to update cni config&quot; err=&quot;no networks found in /etc/cni/net.d&quot;Nov 1 15:04:51 node01 kubelet: I1101 15:04:51.242306 8809 docker_service.go:257] &quot;Docker cri networking managed by the network plugin&quot; networkPluginName=&quot;cni&quot;Nov 1 15:04:51 node01 kubelet: I1101 15:04:51.242331 8809 cni.go:239] &quot;Unable to update cni config&quot; err=&quot;no networks found in /etc/cni/net.d&quot;Nov 1 15:04:51 node01 kubelet: I1101 15:04:51.249360 8809 docker_service.go:264] &quot;Docker Info&quot; dockerInfo=&amp;{ID:ZB4Z:FUQW:IXZR:H3XP:E4PL:WXGO:4ODH:A72V:BDIY:D4AJ:F6S7:M2J2 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:false Debug:false NFd:25 OomKillDisable:true NGoroutines:34 SystemTime:2021-11-01T15:04:51.242765302+08:00 LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:3.10.0-1160.45.1.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSVersion:7 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0003fa070 NCPU:2 MemTotal:2093301760 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:node01 Labels:[] ExperimentalBuild:false ServerVersion:20.10.10 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:&lt;nil&gt;} io.containerd.runtime.v1.linux:{Path:runc Args:[] Shim:&lt;nil&gt;} runc:{Path:runc Args:[] Shim:&lt;nil&gt;}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:&lt;nil&gt; Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:5b46e404f6b9f661a205e28d59c982d3634148f8 Expected:5b46e404f6b9f661a205e28d59c982d3634148f8} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: bridge-nf-call-ip6tables is disabled]}Nov 1 15:04:51 node01 kubelet: E1101 15:04:51.249384 8809 server.go:294] &quot;Failed to run kubelet&quot; err=&quot;failed to run Kubelet: misconfiguration: kubelet cgroup driver: \\&quot;systemd\\&quot; is different from docker cgroup driver: \\&quot;cgroupfs\\&quot;&quot;Nov 1 15:04:51 node01 systemd: kubelet.service: main process exited, code=exited, status=1/FAILURENov 1 15:04:51 node01 systemd: Unit kubelet.service entered failed state.Nov 1 15:04:51 node01 systemd: kubelet.service failed. 发现是 K8S 和 docker 的 cgroup driver 不同。刚才配置了 docker 的 cgroup 的 daemon.json文件没有重启 docker，重启 docker 服务，让配置文件生效。 执行： systemctl daemon-reload &amp;&amp; systemctl restart docker 执行 kubeadm reset 命令回滚通过 kubeadm init 操作产生的文件，然后重新执行 kubeadm init 出现如下的日志，表示 Master 节点 Control Panel 安装成功了： 1234567891011121314151617181920212223242526272829303132[mark-control-plane] Marking the node node01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: 810man.kfqm2lvq8mfqq2k7[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.170.111:6443 --token 810man.kfqm2lvq8mfqq2k7 \\ --discovery-token-ca-cert-hash sha256:e89417229adccca09076f811a36cb50c0e19702c3d2bd0b1a4808c7f68ea1785 根据提示需要配置 CA 证书，如果是 root 用户，直接执行 export KUBECONFIG=/etc/kubernetes/admin.conf，然后就可以使用 kubectl 操作集群了。 目前 Master 节点已经可以工作了，但是没有 Worker 节点，并且现在的集群是没有网络的，需要配置网络。 6. 加入新的 node 节点 安装 kubeadm 和 kubelet 在 worker 节点上无需安装 kubectl yum install kubelet kubeadm --disableexcludes=kubernetes 启动 kubelet 的服务器，并设置为开机启动 systemctl start kubelet &amp;&amp; systemctl enable kubelet kubeadm 需要关闭 linux 的 swap 系统交换区 swapoff -a 修改新 node 节点的 docker cgroup 的配置， /etc/docker/daemon.json 123{ &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;]} 使用 kubeadm join 命令加入集群，可以从上面的安装完 Master 的提示中复制 join 的命令： kubeadm join 192.168.170.111:6443 --token 810man.kfqm2lvq8mfqq2k7 \\ --discovery-token-ca-cert-hash sha256:e89417229adccca09076f811a36cb50c0e19702c3d2bd0b1a4808c7f68ea1785 最后出现日志，This node has joined the cluster xxxxx 123456789101112131415[preflight] Running pre-flight checks [WARNING Hostname]: hostname &quot;node02&quot; could not be reached [WARNING Hostname]: hostname &quot;node02&quot;: lookup node02 on 114.114.114.114:53: no such host[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Starting the kubelet[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 如果执行 kubeadm join 失败，可以执行 kubeadm reset 命令恢复原状，然后再重新执行 join 命令。同理 kubeadm init 命令也是一样。 根据提示，在 Master 节点上执行命令 kubectl get nodes，获取到当前集群中的 node， 1234[root@node01 docker]# kubectl get nodesNAME STATUS ROLES AGE VERSIONnode01 NotReady control-plane,master 67m v1.22.3node02 NotReady &lt;none&gt; 5m38s v1.22.3 这时候是没有网络的，所以节点都是 NOT READY 的状态，在查看 /var/log/messages 的日志时，一直报网络的错误： 12Nov 1 16:18:50 node01 kubelet: I1101 16:18:50.622393 13344 cni.go:239] &quot;Unable to update cni config&quot; err=&quot;no networks found in /etc/cni/net.d&quot;Nov 1 16:18:52 node01 kubelet: E1101 16:18:52.110886 13344 kubelet.go:2337] &quot;Container runtime network not ready&quot; networkReady=&quot;NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized&quot; 所以接下来，我们需要配置网络 CNI 插件。 可以通过设置，让 Master 节点也作为 Node 角色。删除 Node 的 label node-role.kubernetes.io/master即可。通过命令： kubectl taint nodes --all node-role.kubernetes.io/master 7. 安装 CNI 网络插件对于 CNI 网络插件的安装，选择 Calico CNI 插件，在 Master 节点，运行命令一键安装： kubectl apply -f 'https://docs.projectcalico.org/manifests/calico.yaml' 安装完之后，再次执行 kubectl get nodes ，node 已经 READY 状态。 8. 验证 Kurnetes 集群是否正常工作运行查看 POD 命令，验证 K8S 集群服务的 POD 是否创建成功且正常运行 kubectl get pods --all-namespaces，输出如下： 12345678910111213[root@node01 docker]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-kube-controllers-75f8f6cc59-cfwt7 1/1 Running 0 6m19skube-system calico-node-2gb5b 1/1 Running 0 6m19skube-system calico-node-5786l 1/1 Running 0 6m19skube-system coredns-78fcd69978-774r8 1/1 Running 0 93mkube-system coredns-78fcd69978-dgpq4 1/1 Running 0 93mkube-system etcd-node01 1/1 Running 1 93mkube-system kube-apiserver-node01 1/1 Running 1 93mkube-system kube-controller-manager-node01 1/1 Running 1 93mkube-system kube-proxy-59xsr 1/1 Running 0 31mkube-system kube-proxy-kdrt9 1/1 Running 0 93mkube-system kube-scheduler-node01 1/1 Running 1 93m 至此， 一个正常的 K8S 集群就已经安装成功了。但是这个集群是不可靠的集群，一旦 Master 节点 down 调，那么整个集群就会崩溃，所以生产环境中还是要搭建高可用的集群。","link":"/2021-11-%E4%BD%BF%E7%94%A8Kubeadm%E5%B7%A5%E5%85%B7%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85K8S%E9%9B%86%E7%BE%A4/"}],"tags":[{"name":"web","slug":"web","link":"/tags/web/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"reactive","slug":"reactive","link":"/tags/reactive/"},{"name":"AQS","slug":"AQS","link":"/tags/AQS/"},{"name":"JUC","slug":"JUC","link":"/tags/JUC/"},{"name":"CAS","slug":"CAS","link":"/tags/CAS/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"K8S","slug":"K8S","link":"/tags/K8S/"},{"name":"线程池","slug":"线程池","link":"/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"I&#x2F;O","slug":"I-O","link":"/tags/I-O/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"虚拟机","slug":"虚拟机","link":"/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"categories":[{"name":"响应式编程","slug":"响应式编程","link":"/categories/%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"云原生","slug":"云原生","link":"/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"网络编程","slug":"网络编程","link":"/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"多线程","slug":"Java/多线程","link":"/categories/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"容器云","slug":"容器云","link":"/categories/%E5%AE%B9%E5%99%A8%E4%BA%91/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"内存模型","slug":"Java/内存模型","link":"/categories/Java/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"}]}